{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32f8ca24",
   "metadata": {},
   "source": [
    "# Understanding Hired Rides in NYC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f75fd94",
   "metadata": {},
   "source": [
    "## Project Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "66dcde05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all import statements needed for the project\n",
    "\n",
    "import os\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import requests\n",
    "import sqlalchemy as db\n",
    "import re\n",
    "import geopandas as gpd\n",
    "from math import ceil\n",
    "from urllib.parse import unquote\n",
    "import glob\n",
    "from sqlalchemy import text\n",
    "import folium\n",
    "from folium.plugins import HeatMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1242c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "TLC_URL = \"https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page\"\n",
    "\n",
    "dataset_directory = \"/Users/shaoziheng/Desktop/4501/project/datasets\"\n",
    "TAXI_ZONES_DIR = \"/Users/shaoziheng/Desktop/4501/project/datasets/taxi_zones\"\n",
    "TAXI_ZONES_SHAPEFILE = f\"{TAXI_ZONES_DIR}/taxi_zones.shp\"\n",
    "WEATHER_CSV_DIR = \"/Users/shaoziheng/Desktop/4501/project/datasets/weather\"\n",
    "\n",
    "CRS = 4326  # coordinate reference system\n",
    "\n",
    "# (lat, lon)\n",
    "NEW_YORK_BOX_COORDS = ((40.560445, -74.242330), (40.908524, -73.717047))\n",
    "LGA_BOX_COORDS = ((40.763589, -73.891745), (40.778865, -73.854838))\n",
    "JFK_BOX_COORDS = ((40.639263, -73.795642), (40.651376, -73.766264))\n",
    "EWR_BOX_COORDS = ((40.686794, -74.194028), (40.699680, -74.165205))\n",
    "\n",
    "DATABASE_URL = \"sqlite:///project.db\"\n",
    "DATABASE_SCHEMA_FILE = \"schema.sql\"\n",
    "QUERY_DIRECTORY = \"queries\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d6601633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the QUERY_DIRECTORY exists\n",
    "try:\n",
    "    os.mkdir(QUERY_DIRECTORY)\n",
    "except Exception as e:\n",
    "    if e.errno == 17:\n",
    "        # the directory already exists\n",
    "        pass\n",
    "    else:\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ad10ea",
   "metadata": {},
   "source": [
    "## Part 1: Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d53e24",
   "metadata": {},
   "source": [
    "### Load Taxi Zones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "58708809",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_taxi_zones(shapefile):\n",
    "    \"\"\"\n",
    "    Load and preprocess a shapefile containing taxi zone data.\n",
    "\n",
    "    Args:\n",
    "        shapefile (str): Path to the shapefile containing taxi zone boundaries. \n",
    "                         The shapefile must include `LocationID` or similar \n",
    "                         geographic attributes.\n",
    "                        \n",
    "    Returns:\n",
    "        A GeoDataFrame with added `longitude` and \n",
    "        `latitude` columns corresponding to the centroids\n",
    "        of the taxi zones.\n",
    "    \"\"\"\n",
    "    g = gpd.read_file(shapefile)\n",
    "    g = g.to_crs(4326)\n",
    "    g['longitude'] = g.centroid.x\n",
    "    g['latitude'] = g.centroid.y\n",
    "    return g"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc33eed4-b2e9-4ab3-94a8-59f04e464c98",
   "metadata": {},
   "source": [
    "### Web scraping links for downloading files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1dd682b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch URL from the TLC page\n",
    "def get_all_urls_from_tlc_page(taxi_page):\n",
    "    \"\"\"\n",
    "    Fetch the HTML content from the provided TLC page URL and parse it with BeautifulSoup.\n",
    "\n",
    "    Args:\n",
    "        taxi_page (str): The URL of the TLC webpage containing the data links.\n",
    "\n",
    "    Returns:\n",
    "        BeautifulSoup: A parsed BeautifulSoup object containing the HTML content \n",
    "                       of the webpage for further processing.\n",
    "    \"\"\"\n",
    "    response = requests.get(taxi_page)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"Failed to fetch page content: {response.status_code}\")\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cbd0d198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract URLs for yellow taxi and HVFHV data from the TLC page\n",
    "\n",
    "def filter_parquet_urls(soup):\n",
    "    \"\"\"\n",
    "    Extract URLs for yellow taxi and HVFHV trip data in Parquet format from the TLC webpage.\n",
    "\n",
    "    Args:\n",
    "        soup (BeautifulSoup): A BeautifulSoup object containing the parsed HTML of the TLC webpage.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing two lists:\n",
    "            - yellow_taxi_links (list): List of URLs for yellow taxi Parquet files.\n",
    "            - hvfhv_links (list): List of URLs for HVFHV Parquet files.\n",
    "\n",
    "    Description:\n",
    "        - Identifies links that match the naming pattern for yellow taxi and HVFHV trip data.\n",
    "        - Decodes encoded characters (e.g., `%20` -> space) in the URLs.\n",
    "        - Filters links based on file naming conventions for years 2020-2024.\n",
    "    \"\"\"\n",
    "    yellow_taxi_links = []\n",
    "    yellow_links = soup.find_all('a', {'href': re.compile(r\"yellow_tripdata_202[0-3]-\\d{2}\\.parquet|yellow_tripdata_2024-(0[1-8])\\.parquet\")})\n",
    "    for link in yellow_links:\n",
    "        url = link['href'].strip()  # Remove leading/trailing spaces\n",
    "        url = unquote(url)  # Decode any encoded characters like %20\n",
    "        yellow_taxi_links.append(url)\n",
    "        \n",
    "    hvfhv_links = []\n",
    "    hvfhv_links_soup = soup.find_all('a', {'href': re.compile(r\"fhvhv_tripdata_202[0-3]-\\d{2}\\.parquet|fhvhv_tripdata_2024-(0[1-8])\\.parquet\")})\n",
    "    for link in hvfhv_links_soup:\n",
    "        url = link['href'].strip()  # Remove leading/trailing spaces\n",
    "        url = unquote(url)  # Decode any encoded characters like %20\n",
    "        hvfhv_links.append(url)\n",
    "        \n",
    "    return yellow_taxi_links, hvfhv_links\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f9779b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download  the Yellow Taxi & High-Volume For-Hire Vehicle (HVFHV) trip data parquet files and save them to directory\n",
    "\n",
    "def download_parquet_file(urls, output_directory):\n",
    "    \"\"\"\n",
    "    Downloads Parquet files from a list of URLs and saves them to the specified directory.\n",
    "\n",
    "    Args:\n",
    "        urls (list): List of URLs to download.\n",
    "        output_directory (str): Path to the directory where files will be saved.\n",
    "\n",
    "    Raises:\n",
    "        Exception: If a file fails to download.\n",
    "    \"\"\"\n",
    "    # Ensure the output directory exists\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "    for url in urls:\n",
    "        file_name = os.path.basename(url)\n",
    "        output_path = os.path.join(output_directory, file_name)\n",
    "\n",
    "        try:\n",
    "            print(f\"Downloading {url}...\")\n",
    "            response = requests.get(url, stream=True)\n",
    "            response.raise_for_status()  # Raise an error for failed requests\n",
    "\n",
    "            # Write the file content to disk\n",
    "            with open(output_path, 'wb') as file:\n",
    "                for chunk in response.iter_content(chunk_size=1024):\n",
    "                    if chunk:  # Filter out keep-alive chunks\n",
    "                        file.write(chunk)\n",
    "\n",
    "            print(f\"Saved to {output_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to download {url}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bc350086",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup=get_all_urls_from_tlc_page(TLC_URL)\n",
    "yellow_taxi_links, hvfhv_links = filter_parquet_urls(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "030a4830",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[WinError 5] Access is denied: '/Users/shaoziheng'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Download Yellow Taxi files\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m download_parquet_file(yellow_taxi_links, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(dataset_directory, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myellow_tripdata\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Download Uber HVFHV files\u001b[39;00m\n\u001b[0;32m      5\u001b[0m download_parquet_file(hvfhv_links, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(dataset_directory, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfhvhv_tripdata\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "Cell \u001b[1;32mIn[37], line 15\u001b[0m, in \u001b[0;36mdownload_parquet_file\u001b[1;34m(urls, output_directory)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;124;03mDownloads Parquet files from a list of URLs and saves them to the specified directory.\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;124;03m    Exception: If a file fails to download.\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Ensure the output directory exists\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(output_directory, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m url \u001b[38;5;129;01min\u001b[39;00m urls:\n\u001b[0;32m     18\u001b[0m     file_name \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(url)\n",
      "File \u001b[1;32m<frozen os>:215\u001b[0m, in \u001b[0;36mmakedirs\u001b[1;34m(name, mode, exist_ok)\u001b[0m\n",
      "File \u001b[1;32m<frozen os>:215\u001b[0m, in \u001b[0;36mmakedirs\u001b[1;34m(name, mode, exist_ok)\u001b[0m\n",
      "    \u001b[1;31m[... skipping similar frames: makedirs at line 215 (3 times)]\u001b[0m\n",
      "File \u001b[1;32m<frozen os>:215\u001b[0m, in \u001b[0;36mmakedirs\u001b[1;34m(name, mode, exist_ok)\u001b[0m\n",
      "File \u001b[1;32m<frozen os>:225\u001b[0m, in \u001b[0;36mmakedirs\u001b[1;34m(name, mode, exist_ok)\u001b[0m\n",
      "\u001b[1;31mPermissionError\u001b[0m: [WinError 5] Access is denied: '/Users/shaoziheng'"
     ]
    }
   ],
   "source": [
    "# Download Yellow Taxi files\n",
    "download_parquet_file(yellow_taxi_links, os.path.join(dataset_directory, \"yellow_tripdata\"))\n",
    "\n",
    "# Download Uber HVFHV files\n",
    "download_parquet_file(hvfhv_links, os.path.join(dataset_directory, \"fhvhv_tripdata\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c0ea4b",
   "metadata": {},
   "source": [
    "### Calculate Sample Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4ddcbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate sample size using Cochran's formula with 95% CI and 5% marginal error.\n",
    "\n",
    "def calculate_sample_size(population, confidence_level=0.95, margin_of_error=0.05):\n",
    "    \"\"\"\n",
    "    Calculate the sample size using Cochran's formula, considering finite population correction.\n",
    "\n",
    "    Args:\n",
    "        population (int): Total population size for which the sample size needs to be calculated.\n",
    "        confidence_level (float): Desired confidence level (default is 0.95 for 95% CI).\n",
    "        margin_of_error (float): Allowable margin of error (default is 0.05 for 5%).\n",
    "\n",
    "    Returns:\n",
    "        int: The calculated sample size, rounded up to the nearest integer.\n",
    "\n",
    "    Formula:\n",
    "        Cochran's formula for infinite population:\n",
    "            n0 = (Z^2 * p * (1 - p)) / e^2\n",
    "        Where:\n",
    "            - Z: Z-score corresponding to the confidence level.\n",
    "            - p: Estimated proportion of the population (default: 0.5 for maximum variability).\n",
    "            - e: Margin of error.\n",
    "\n",
    "        Finite population correction for population size N:\n",
    "            n = n0 / (1 + (n0 - 1) / N)\n",
    "    \"\"\"\n",
    "    Z = {0.9: 1.645, 0.95: 1.96, 0.99: 2.576}[confidence_level]\n",
    "    p = 0.5\n",
    "    e = margin_of_error\n",
    "    sample_size = (Z**2 * p * (1 - p)) / e**2\n",
    "    if population < 1e6:  # Finite population correction\n",
    "        sample_size = sample_size / (1 + (sample_size - 1) / population)\n",
    "    return ceil(sample_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b2d57c",
   "metadata": {},
   "source": [
    "### Process Taxi Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b71420",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_cleaning_and_combine_taxi(input_directory, output_file):\n",
    "    \"\"\"\n",
    "    Apply the `clean_taxi_month` function to all Parquet files in the input directory\n",
    "    and combine the cleaned and sampled files into a single large Parquet file.\n",
    "\n",
    "    Args:\n",
    "        input_directory (str): Path to the directory containing the Parquet files.\n",
    "        output_file (str): Path to save the combined cleaned Parquet file.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Get all Parquet files in the directory\n",
    "    parquet_files = glob.glob(f\"{input_directory}/*.parquet\")\n",
    "    \n",
    "    all_cleaned_data = []  # To store all cleaned DataFrames\n",
    "\n",
    "    # Apply cleaning to each file\n",
    "    for file_path in parquet_files:\n",
    "        print(f\"Processing file: {file_path}\")\n",
    "        \n",
    "        # load taxi zone data\n",
    "        taxi_zones = load_taxi_zones(TAXI_ZONES_SHAPEFILE)\n",
    "        taxi_zones = taxi_zones[['LocationID', 'longitude', 'latitude', 'zone', 'borough']]\n",
    "        df = pd.read_parquet(file_path)\n",
    "        lat_map = dict(zip(taxi_zones['LocationID'], taxi_zones['latitude']))\n",
    "        lon_map = dict(zip(taxi_zones['LocationID'], taxi_zones['longitude']))\n",
    "        \n",
    "        # Apply the transformations from clean_taxi_month\n",
    "        df['pickup_latitude'] = df['PULocationID'].map(lat_map)\n",
    "        df['pickup_longitude'] = df['PULocationID'].map(lon_map)\n",
    "        df['dropoff_latitude'] = df['DOLocationID'].map(lat_map)\n",
    "        df['dropoff_longitude'] = df['DOLocationID'].map(lon_map)\n",
    "\n",
    "        # remove invalid location IDs - drop rows where any of the values are missing \n",
    "        df = df.dropna(subset=['pickup_latitude', 'pickup_longitude', 'dropoff_latitude', 'dropoff_longitude'])\n",
    "        \n",
    "        # Filter out invalid data points (non-positive fare, tip, passenger, trip distance)\n",
    "        df = df[(df['fare_amount'] > 0) & (df['tip_amount'] >= 0) & \n",
    "                (df['total_amount'] >= 0) & (df['passenger_count'] > 0) & \n",
    "                (df['trip_distance'] > 0)]\n",
    "        \n",
    "\n",
    "        # remove rows where pickup and dropoff latitudes/longitudes are the same or very close\n",
    "        df = df[(df['pickup_latitude'] != df['dropoff_latitude']) & \n",
    "                (df['pickup_longitude'] != df['dropoff_longitude']) & \n",
    "                (abs(df['pickup_longitude'] - df['dropoff_longitude']) > 0.001)]\n",
    "        \n",
    "        # Normalize column names\n",
    "        df.columns = [col.lower().strip().replace(' ', '_') for col in df.columns]\n",
    "\n",
    "        # Remove trips that start and/or end outside of the NEW_YORK_BOX\n",
    "        NEW_YORK_BOX_COORDS = [(40.560445, -74.242330), (40.908524, -73.717047)]\n",
    "        df = df[(df['pickup_latitude'] >= NEW_YORK_BOX_COORDS[0][0]) & \n",
    "                (df['pickup_latitude'] <= NEW_YORK_BOX_COORDS[1][0]) & \n",
    "                (df['pickup_longitude'] >= NEW_YORK_BOX_COORDS[0][1]) & \n",
    "                (df['pickup_longitude'] <= NEW_YORK_BOX_COORDS[1][1])]\n",
    "        df = df[(df['dropoff_latitude'] >= NEW_YORK_BOX_COORDS[0][0]) & \n",
    "                (df['dropoff_latitude'] <= NEW_YORK_BOX_COORDS[1][0]) & \n",
    "                (df['dropoff_longitude'] >= NEW_YORK_BOX_COORDS[0][1]) & \n",
    "                (df['dropoff_longitude'] <= NEW_YORK_BOX_COORDS[1][1])]\n",
    "        \n",
    "        # remove unnecessary columns \n",
    "        df = df[['tpep_pickup_datetime','tpep_dropoff_datetime','fare_amount','tip_amount', 'extra', 'improvement_surcharge',\n",
    "                  'congestion_surcharge', 'airport_fee', 'mta_tax', 'tolls_amount', 'pickup_latitude','pickup_longitude',\n",
    "                 'dropoff_latitude','dropoff_longitude', 'trip_distance'\n",
    "                ]]\n",
    "        \n",
    "        # Convert data types after filtering\n",
    "        df['tpep_pickup_datetime'] = pd.to_datetime(df['tpep_pickup_datetime'], errors='coerce')\n",
    "        df['tpep_dropoff_datetime'] = pd.to_datetime(df['tpep_dropoff_datetime'], errors='coerce')\n",
    "        df['fare_amount'] = df['fare_amount'].astype(float, errors='ignore')\n",
    "        df['tip_amount'] = df['tip_amount'].astype(float, errors='ignore')\n",
    "        df['extra'] = df['extra'].astype(float, errors='ignore')\n",
    "        df['improvement_surcharge'] = df['improvement_surcharge'].astype(float, errors='ignore')\n",
    "        df['congestion_surcharge'] = df['congestion_surcharge'].astype(float, errors='ignore')\n",
    "        df['airport_fee'] = df['airport_fee'].fillna(0).astype(float, errors='ignore')\n",
    "        df['mta_tax'] = df['mta_tax'].astype(float, errors='ignore')\n",
    "        df['tolls_amount'] = df['tolls_amount'].astype(float, errors='ignore')\n",
    "        df['pickup_latitude'] = df['pickup_latitude'].astype(float, errors='ignore')\n",
    "        df['pickup_longitude'] = df['pickup_longitude'].astype(float, errors='ignore')\n",
    "        df['dropoff_latitude'] = df['dropoff_latitude'].astype(float, errors='ignore')\n",
    "        df['dropoff_longitude'] = df['dropoff_longitude'].astype(float, errors='ignore')\n",
    "        df['trip_distance'] = df['trip_distance'].astype(float, errors='ignore')\n",
    "        \n",
    "        \n",
    "        \n",
    "        # calculate sample size and generate sampled_df\n",
    "        population_size = len(df)\n",
    "        sample_size = calculate_sample_size(population_size, confidence_level=0.95, margin_of_error=0.05)\n",
    "        sampled_df = df.sample(n=sample_size, random_state=42)\n",
    "\n",
    "        # Append cleaned and sampled data to the list\n",
    "        all_cleaned_data.append(sampled_df)\n",
    "        print(f\"Finished processing file: {file_path}\")\n",
    "    \n",
    "    # Combine all cleaned DataFrames\n",
    "    combined_df = pd.concat(all_cleaned_data, ignore_index=True)\n",
    "    print(f\"Combined all cleaned data into a single DataFrame with {len(combined_df)} rows.\")\n",
    "\n",
    "    # Save the combined DataFrame to a Parquet file\n",
    "    combined_df.to_parquet(output_file, index=False)\n",
    "    print(f\"Saved combined data to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d140f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_input_directory = f\"{dataset_directory}/yellow_tripdata\"\n",
    "taxi_output_file = f\"{dataset_directory}/combined_taxi_data.parquet\"\n",
    "apply_cleaning_and_combine_taxi(taxi_input_directory, taxi_output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb656e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_data=pd.read_parquet(taxi_output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c958e185",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tpep_pickup_datetime</th>\n",
       "      <th>tpep_dropoff_datetime</th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>tip_amount</th>\n",
       "      <th>extra</th>\n",
       "      <th>improvement_surcharge</th>\n",
       "      <th>congestion_surcharge</th>\n",
       "      <th>airport_fee</th>\n",
       "      <th>mta_tax</th>\n",
       "      <th>tolls_amount</th>\n",
       "      <th>pickup_latitude</th>\n",
       "      <th>pickup_longitude</th>\n",
       "      <th>dropoff_latitude</th>\n",
       "      <th>dropoff_longitude</th>\n",
       "      <th>trip_distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-06-27 16:50:19</td>\n",
       "      <td>2023-06-27 16:55:31</td>\n",
       "      <td>6.5</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>40.756687</td>\n",
       "      <td>-73.972356</td>\n",
       "      <td>40.758027</td>\n",
       "      <td>-73.977698</td>\n",
       "      <td>0.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-06-28 14:01:22</td>\n",
       "      <td>2023-06-28 14:16:21</td>\n",
       "      <td>19.8</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>40.706808</td>\n",
       "      <td>-74.007496</td>\n",
       "      <td>40.734575</td>\n",
       "      <td>-74.002875</td>\n",
       "      <td>3.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-06-09 20:04:55</td>\n",
       "      <td>2023-06-09 20:12:42</td>\n",
       "      <td>9.3</td>\n",
       "      <td>3.16</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>40.748427</td>\n",
       "      <td>-73.999917</td>\n",
       "      <td>40.735035</td>\n",
       "      <td>-74.008984</td>\n",
       "      <td>1.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-06-04 01:03:00</td>\n",
       "      <td>2023-06-04 01:08:15</td>\n",
       "      <td>6.5</td>\n",
       "      <td>2.00</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>40.734575</td>\n",
       "      <td>-74.002875</td>\n",
       "      <td>40.723888</td>\n",
       "      <td>-74.001537</td>\n",
       "      <td>0.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-06-30 08:07:36</td>\n",
       "      <td>2023-06-30 08:13:14</td>\n",
       "      <td>7.9</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>40.780436</td>\n",
       "      <td>-73.957011</td>\n",
       "      <td>40.790010</td>\n",
       "      <td>-73.945750</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21550</th>\n",
       "      <td>2023-03-01 22:21:47</td>\n",
       "      <td>2023-03-01 22:36:55</td>\n",
       "      <td>19.8</td>\n",
       "      <td>6.27</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>6.55</td>\n",
       "      <td>40.753512</td>\n",
       "      <td>-73.988786</td>\n",
       "      <td>40.729506</td>\n",
       "      <td>-73.949540</td>\n",
       "      <td>3.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21551</th>\n",
       "      <td>2023-03-01 15:57:02</td>\n",
       "      <td>2023-03-01 16:06:07</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>40.818257</td>\n",
       "      <td>-73.940771</td>\n",
       "      <td>40.801169</td>\n",
       "      <td>-73.937345</td>\n",
       "      <td>0.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21552</th>\n",
       "      <td>2023-03-06 17:46:31</td>\n",
       "      <td>2023-03-06 17:53:27</td>\n",
       "      <td>8.6</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>40.732579</td>\n",
       "      <td>-73.994305</td>\n",
       "      <td>40.742278</td>\n",
       "      <td>-73.996971</td>\n",
       "      <td>1.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21553</th>\n",
       "      <td>2023-03-22 17:44:43</td>\n",
       "      <td>2023-03-22 17:55:28</td>\n",
       "      <td>10.7</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>40.775965</td>\n",
       "      <td>-73.987645</td>\n",
       "      <td>40.762252</td>\n",
       "      <td>-73.989844</td>\n",
       "      <td>1.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21554</th>\n",
       "      <td>2023-03-15 20:53:02</td>\n",
       "      <td>2023-03-15 20:59:44</td>\n",
       "      <td>9.3</td>\n",
       "      <td>2.85</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>40.766948</td>\n",
       "      <td>-73.959635</td>\n",
       "      <td>40.749913</td>\n",
       "      <td>-73.970442</td>\n",
       "      <td>1.40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21555 rows Ã— 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      tpep_pickup_datetime tpep_dropoff_datetime  fare_amount  tip_amount  \\\n",
       "0      2023-06-27 16:50:19   2023-06-27 16:55:31          6.5        1.00   \n",
       "1      2023-06-28 14:01:22   2023-06-28 14:16:21         19.8        0.00   \n",
       "2      2023-06-09 20:04:55   2023-06-09 20:12:42          9.3        3.16   \n",
       "3      2023-06-04 01:03:00   2023-06-04 01:08:15          6.5        2.00   \n",
       "4      2023-06-30 08:07:36   2023-06-30 08:13:14          7.9        1.00   \n",
       "...                    ...                   ...          ...         ...   \n",
       "21550  2023-03-01 22:21:47   2023-03-01 22:36:55         19.8        6.27   \n",
       "21551  2023-03-01 15:57:02   2023-03-01 16:06:07         10.0        0.00   \n",
       "21552  2023-03-06 17:46:31   2023-03-06 17:53:27          8.6        0.00   \n",
       "21553  2023-03-22 17:44:43   2023-03-22 17:55:28         10.7        0.00   \n",
       "21554  2023-03-15 20:53:02   2023-03-15 20:59:44          9.3        2.85   \n",
       "\n",
       "       extra  improvement_surcharge  congestion_surcharge  airport_fee  \\\n",
       "0        2.5                    1.0                   2.5          0.0   \n",
       "1        0.0                    1.0                   2.5          0.0   \n",
       "2        2.5                    1.0                   2.5          0.0   \n",
       "3        3.5                    1.0                   2.5          0.0   \n",
       "4        2.5                    1.0                   2.5          0.0   \n",
       "...      ...                    ...                   ...          ...   \n",
       "21550    1.0                    1.0                   2.5          0.0   \n",
       "21551    0.0                    1.0                   0.0          0.0   \n",
       "21552    2.5                    1.0                   2.5          0.0   \n",
       "21553    5.0                    1.0                   2.5          0.0   \n",
       "21554    3.5                    1.0                   2.5          0.0   \n",
       "\n",
       "       mta_tax  tolls_amount  pickup_latitude  pickup_longitude  \\\n",
       "0          0.5          0.00        40.756687        -73.972356   \n",
       "1          0.5          0.00        40.706808        -74.007496   \n",
       "2          0.5          0.00        40.748427        -73.999917   \n",
       "3          0.5          0.00        40.734575        -74.002875   \n",
       "4          0.5          0.00        40.780436        -73.957011   \n",
       "...        ...           ...              ...               ...   \n",
       "21550      0.5          6.55        40.753512        -73.988786   \n",
       "21551      0.5          0.00        40.818257        -73.940771   \n",
       "21552      0.5          0.00        40.732579        -73.994305   \n",
       "21553      0.5          0.00        40.775965        -73.987645   \n",
       "21554      0.5          0.00        40.766948        -73.959635   \n",
       "\n",
       "       dropoff_latitude  dropoff_longitude  trip_distance  \n",
       "0             40.758027         -73.977698           0.70  \n",
       "1             40.734575         -74.002875           3.73  \n",
       "2             40.735035         -74.008984           1.06  \n",
       "3             40.723888         -74.001537           0.70  \n",
       "4             40.790010         -73.945750           0.80  \n",
       "...                 ...                ...            ...  \n",
       "21550         40.729506         -73.949540           3.94  \n",
       "21551         40.801169         -73.937345           0.88  \n",
       "21552         40.742278         -73.996971           1.07  \n",
       "21553         40.762252         -73.989844           1.10  \n",
       "21554         40.749913         -73.970442           1.40  \n",
       "\n",
       "[21555 rows x 15 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxi_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7bbec8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tpep_pickup_datetime</th>\n",
       "      <th>tpep_dropoff_datetime</th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>tip_amount</th>\n",
       "      <th>extra</th>\n",
       "      <th>improvement_surcharge</th>\n",
       "      <th>congestion_surcharge</th>\n",
       "      <th>airport_fee</th>\n",
       "      <th>mta_tax</th>\n",
       "      <th>tolls_amount</th>\n",
       "      <th>pickup_latitude</th>\n",
       "      <th>pickup_longitude</th>\n",
       "      <th>dropoff_latitude</th>\n",
       "      <th>dropoff_longitude</th>\n",
       "      <th>trip_distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-06-27 16:50:19</td>\n",
       "      <td>2023-06-27 16:55:31</td>\n",
       "      <td>6.5</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.756687</td>\n",
       "      <td>-73.972356</td>\n",
       "      <td>40.758027</td>\n",
       "      <td>-73.977698</td>\n",
       "      <td>0.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-06-28 14:01:22</td>\n",
       "      <td>2023-06-28 14:16:21</td>\n",
       "      <td>19.8</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.706808</td>\n",
       "      <td>-74.007496</td>\n",
       "      <td>40.734575</td>\n",
       "      <td>-74.002875</td>\n",
       "      <td>3.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-06-09 20:04:55</td>\n",
       "      <td>2023-06-09 20:12:42</td>\n",
       "      <td>9.3</td>\n",
       "      <td>3.16</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.748427</td>\n",
       "      <td>-73.999917</td>\n",
       "      <td>40.735035</td>\n",
       "      <td>-74.008984</td>\n",
       "      <td>1.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-06-04 01:03:00</td>\n",
       "      <td>2023-06-04 01:08:15</td>\n",
       "      <td>6.5</td>\n",
       "      <td>2.00</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.734575</td>\n",
       "      <td>-74.002875</td>\n",
       "      <td>40.723888</td>\n",
       "      <td>-74.001537</td>\n",
       "      <td>0.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-06-30 08:07:36</td>\n",
       "      <td>2023-06-30 08:13:14</td>\n",
       "      <td>7.9</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.780436</td>\n",
       "      <td>-73.957011</td>\n",
       "      <td>40.790010</td>\n",
       "      <td>-73.945750</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  tpep_pickup_datetime tpep_dropoff_datetime  fare_amount  tip_amount  extra  \\\n",
       "0  2023-06-27 16:50:19   2023-06-27 16:55:31          6.5        1.00    2.5   \n",
       "1  2023-06-28 14:01:22   2023-06-28 14:16:21         19.8        0.00    0.0   \n",
       "2  2023-06-09 20:04:55   2023-06-09 20:12:42          9.3        3.16    2.5   \n",
       "3  2023-06-04 01:03:00   2023-06-04 01:08:15          6.5        2.00    3.5   \n",
       "4  2023-06-30 08:07:36   2023-06-30 08:13:14          7.9        1.00    2.5   \n",
       "\n",
       "   improvement_surcharge  congestion_surcharge  airport_fee  mta_tax  \\\n",
       "0                    1.0                   2.5          0.0      0.5   \n",
       "1                    1.0                   2.5          0.0      0.5   \n",
       "2                    1.0                   2.5          0.0      0.5   \n",
       "3                    1.0                   2.5          0.0      0.5   \n",
       "4                    1.0                   2.5          0.0      0.5   \n",
       "\n",
       "   tolls_amount  pickup_latitude  pickup_longitude  dropoff_latitude  \\\n",
       "0           0.0        40.756687        -73.972356         40.758027   \n",
       "1           0.0        40.706808        -74.007496         40.734575   \n",
       "2           0.0        40.748427        -73.999917         40.735035   \n",
       "3           0.0        40.734575        -74.002875         40.723888   \n",
       "4           0.0        40.780436        -73.957011         40.790010   \n",
       "\n",
       "   dropoff_longitude  trip_distance  \n",
       "0         -73.977698           0.70  \n",
       "1         -74.002875           3.73  \n",
       "2         -74.008984           1.06  \n",
       "3         -74.001537           0.70  \n",
       "4         -73.945750           0.80  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxi_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61f15b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 21555 entries, 0 to 21554\n",
      "Data columns (total 15 columns):\n",
      " #   Column                 Non-Null Count  Dtype         \n",
      "---  ------                 --------------  -----         \n",
      " 0   tpep_pickup_datetime   21555 non-null  datetime64[ns]\n",
      " 1   tpep_dropoff_datetime  21555 non-null  datetime64[ns]\n",
      " 2   fare_amount            21555 non-null  float64       \n",
      " 3   tip_amount             21555 non-null  float64       \n",
      " 4   extra                  21555 non-null  float64       \n",
      " 5   improvement_surcharge  21555 non-null  float64       \n",
      " 6   congestion_surcharge   21555 non-null  float64       \n",
      " 7   airport_fee            21555 non-null  float64       \n",
      " 8   mta_tax                21555 non-null  float64       \n",
      " 9   tolls_amount           21555 non-null  float64       \n",
      " 10  pickup_latitude        21555 non-null  float64       \n",
      " 11  pickup_longitude       21555 non-null  float64       \n",
      " 12  dropoff_latitude       21555 non-null  float64       \n",
      " 13  dropoff_longitude      21555 non-null  float64       \n",
      " 14  trip_distance          21555 non-null  float64       \n",
      "dtypes: datetime64[ns](2), float64(13)\n",
      "memory usage: 2.5 MB\n"
     ]
    }
   ],
   "source": [
    "taxi_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1c6450",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tpep_pickup_datetime</th>\n",
       "      <th>tpep_dropoff_datetime</th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>tip_amount</th>\n",
       "      <th>extra</th>\n",
       "      <th>improvement_surcharge</th>\n",
       "      <th>congestion_surcharge</th>\n",
       "      <th>airport_fee</th>\n",
       "      <th>mta_tax</th>\n",
       "      <th>tolls_amount</th>\n",
       "      <th>pickup_latitude</th>\n",
       "      <th>pickup_longitude</th>\n",
       "      <th>dropoff_latitude</th>\n",
       "      <th>dropoff_longitude</th>\n",
       "      <th>trip_distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>21555</td>\n",
       "      <td>21555</td>\n",
       "      <td>21555.000000</td>\n",
       "      <td>21555.000000</td>\n",
       "      <td>21555.000000</td>\n",
       "      <td>21555.000000</td>\n",
       "      <td>21555.000000</td>\n",
       "      <td>21555.000000</td>\n",
       "      <td>21555.000000</td>\n",
       "      <td>21555.000000</td>\n",
       "      <td>21555.000000</td>\n",
       "      <td>21555.000000</td>\n",
       "      <td>21555.000000</td>\n",
       "      <td>21555.000000</td>\n",
       "      <td>21555.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2022-05-02 02:22:55.033170688</td>\n",
       "      <td>2022-05-02 02:40:03.177824256</td>\n",
       "      <td>15.498320</td>\n",
       "      <td>2.817350</td>\n",
       "      <td>1.273732</td>\n",
       "      <td>0.553250</td>\n",
       "      <td>2.326374</td>\n",
       "      <td>0.090478</td>\n",
       "      <td>0.498056</td>\n",
       "      <td>0.469210</td>\n",
       "      <td>40.753237</td>\n",
       "      <td>-73.967072</td>\n",
       "      <td>40.755621</td>\n",
       "      <td>-73.971460</td>\n",
       "      <td>3.319831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2020-01-01 03:06:55</td>\n",
       "      <td>2020-01-01 03:13:30</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>40.576961</td>\n",
       "      <td>-74.174000</td>\n",
       "      <td>40.571769</td>\n",
       "      <td>-74.174000</td>\n",
       "      <td>0.010000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2021-03-01 17:26:52</td>\n",
       "      <td>2021-03-01 17:35:34</td>\n",
       "      <td>7.500000</td>\n",
       "      <td>0.180000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>40.740438</td>\n",
       "      <td>-73.989844</td>\n",
       "      <td>40.740337</td>\n",
       "      <td>-73.989844</td>\n",
       "      <td>1.160000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2022-05-01 11:01:50</td>\n",
       "      <td>2022-05-01 11:21:35</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>2.260000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>40.758027</td>\n",
       "      <td>-73.977698</td>\n",
       "      <td>40.758027</td>\n",
       "      <td>-73.977698</td>\n",
       "      <td>1.870000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2023-07-01 00:33:29.500000</td>\n",
       "      <td>2023-07-01 00:48:51.500000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>3.560000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>40.773633</td>\n",
       "      <td>-73.965146</td>\n",
       "      <td>40.774375</td>\n",
       "      <td>-73.959635</td>\n",
       "      <td>3.320000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2024-08-31 21:58:30</td>\n",
       "      <td>2024-08-31 22:21:51</td>\n",
       "      <td>159.800000</td>\n",
       "      <td>99.990000</td>\n",
       "      <td>11.750000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>1.750000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>40.897932</td>\n",
       "      <td>-73.739473</td>\n",
       "      <td>40.899529</td>\n",
       "      <td>-73.735555</td>\n",
       "      <td>57.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.730091</td>\n",
       "      <td>3.239298</td>\n",
       "      <td>1.530403</td>\n",
       "      <td>0.336887</td>\n",
       "      <td>0.635560</td>\n",
       "      <td>0.361685</td>\n",
       "      <td>0.031472</td>\n",
       "      <td>1.891135</td>\n",
       "      <td>0.031175</td>\n",
       "      <td>0.045508</td>\n",
       "      <td>0.032247</td>\n",
       "      <td>0.035536</td>\n",
       "      <td>4.106375</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                tpep_pickup_datetime          tpep_dropoff_datetime  \\\n",
       "count                          21555                          21555   \n",
       "mean   2022-05-02 02:22:55.033170688  2022-05-02 02:40:03.177824256   \n",
       "min              2020-01-01 03:06:55            2020-01-01 03:13:30   \n",
       "25%              2021-03-01 17:26:52            2021-03-01 17:35:34   \n",
       "50%              2022-05-01 11:01:50            2022-05-01 11:21:35   \n",
       "75%       2023-07-01 00:33:29.500000     2023-07-01 00:48:51.500000   \n",
       "max              2024-08-31 21:58:30            2024-08-31 22:21:51   \n",
       "std                              NaN                            NaN   \n",
       "\n",
       "        fare_amount    tip_amount         extra  improvement_surcharge  \\\n",
       "count  21555.000000  21555.000000  21555.000000           21555.000000   \n",
       "mean      15.498320      2.817350      1.273732               0.553250   \n",
       "min        0.010000      0.000000      0.000000               0.000000   \n",
       "25%        7.500000      0.180000      0.000000               0.300000   \n",
       "50%       11.000000      2.260000      1.000000               0.300000   \n",
       "75%       17.000000      3.560000      2.500000               1.000000   \n",
       "max      159.800000     99.990000     11.750000               1.000000   \n",
       "std       13.730091      3.239298      1.530403               0.336887   \n",
       "\n",
       "       congestion_surcharge   airport_fee       mta_tax  tolls_amount  \\\n",
       "count          21555.000000  21555.000000  21555.000000  21555.000000   \n",
       "mean               2.326374      0.090478      0.498056      0.469210   \n",
       "min                0.000000      0.000000      0.000000      0.000000   \n",
       "25%                2.500000      0.000000      0.500000      0.000000   \n",
       "50%                2.500000      0.000000      0.500000      0.000000   \n",
       "75%                2.500000      0.000000      0.500000      0.000000   \n",
       "max                2.500000      1.750000      0.800000     40.000000   \n",
       "std                0.635560      0.361685      0.031472      1.891135   \n",
       "\n",
       "       pickup_latitude  pickup_longitude  dropoff_latitude  dropoff_longitude  \\\n",
       "count     21555.000000      21555.000000      21555.000000       21555.000000   \n",
       "mean         40.753237        -73.967072         40.755621         -73.971460   \n",
       "min          40.576961        -74.174000         40.571769         -74.174000   \n",
       "25%          40.740438        -73.989844         40.740337         -73.989844   \n",
       "50%          40.758027        -73.977698         40.758027         -73.977698   \n",
       "75%          40.773633        -73.965146         40.774375         -73.959635   \n",
       "max          40.897932        -73.739473         40.899529         -73.735555   \n",
       "std           0.031175          0.045508          0.032247           0.035536   \n",
       "\n",
       "       trip_distance  \n",
       "count   21555.000000  \n",
       "mean        3.319831  \n",
       "min         0.010000  \n",
       "25%         1.160000  \n",
       "50%         1.870000  \n",
       "75%         3.320000  \n",
       "max        57.700000  \n",
       "std         4.106375  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxi_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffbf225",
   "metadata": {},
   "source": [
    "### Processing Uber Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49e0785",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_cleaning_and_combine_uber(input_directory, intermediate_directory, output_file):\n",
    "    \"\"\"\n",
    "    Process each Parquet file individually and save cleaned data as intermediate files.\n",
    "    Then combine all intermediate files into a single large Parquet file.\n",
    "\n",
    "    Args:\n",
    "        input_directory (str): Path to the directory containing the Parquet files.\n",
    "        intermediate_directory (str): Path to save intermediate cleaned files.\n",
    "        output_file (str): Path to save the final combined Parquet file.\n",
    "        confidence_level (float): Confidence level for sample size calculation.\n",
    "        margin_of_error (float): Margin of error for sample size calculation.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Ensure the intermediate directory exists\n",
    "    os.makedirs(intermediate_directory, exist_ok=True)\n",
    "\n",
    "    # Get all Parquet files in the directory\n",
    "    parquet_files = glob.glob(f\"{input_directory}/*.parquet\")\n",
    "    \n",
    "    for file_path in parquet_files:\n",
    "        try:\n",
    "            print(f\"Processing file: {file_path}\")\n",
    "            \n",
    "            # Load the file\n",
    "            df = pd.read_parquet(file_path)\n",
    "            \n",
    "            # Filter Uber rides\n",
    "            df = df[df['hvfhs_license_num'] == 'HV0003']\n",
    "\n",
    "            # Load taxi zones for mapping\n",
    "            taxi_zones = load_taxi_zones(TAXI_ZONES_SHAPEFILE)\n",
    "            taxi_zones = taxi_zones[['LocationID', 'longitude', 'latitude', 'zone', 'borough']]\n",
    "            lat_map = dict(zip(taxi_zones['LocationID'], taxi_zones['latitude']))\n",
    "            lon_map = dict(zip(taxi_zones['LocationID'], taxi_zones['longitude']))\n",
    "\n",
    "            # Map latitude and longitude\n",
    "            df['pickup_latitude'] = df['PULocationID'].map(lat_map)\n",
    "            df['pickup_longitude'] = df['PULocationID'].map(lon_map)\n",
    "            df['dropoff_latitude'] = df['DOLocationID'].map(lat_map)\n",
    "            df['dropoff_longitude'] = df['DOLocationID'].map(lon_map)\n",
    "\n",
    "            # Drop rows with missing latitude and longitude\n",
    "            df = df.dropna(subset=['pickup_latitude', 'pickup_longitude', 'dropoff_latitude', 'dropoff_longitude'])\n",
    "\n",
    "            # Filter invalid data points\n",
    "            df = df[(df['trip_miles'] > 0) & (df['trip_time'] > 0) &\n",
    "                    (df['base_passenger_fare'] >= 0) & (df['tolls'] >= 0) &\n",
    "                    (df['bcf'] >= 0) & (df['sales_tax'] >= 0) & (df['tips'] >= 0) &\n",
    "                    (df['driver_pay'] >= 0) & (df['congestion_surcharge'] >= 0)]\n",
    "\n",
    "            # Remove trips that start and/or end outside of the NEW_YORK_BOX\n",
    "            NEW_YORK_BOX_COORDS = [(40.560445, -74.242330), (40.908524, -73.717047)]\n",
    "            df = df[(df['pickup_latitude'] >= NEW_YORK_BOX_COORDS[0][0]) & \n",
    "                    (df['pickup_latitude'] <= NEW_YORK_BOX_COORDS[1][0]) & \n",
    "                    (df['pickup_longitude'] >= NEW_YORK_BOX_COORDS[0][1]) & \n",
    "                    (df['pickup_longitude'] <= NEW_YORK_BOX_COORDS[1][1])]\n",
    "            df = df[(df['dropoff_latitude'] >= NEW_YORK_BOX_COORDS[0][0]) & \n",
    "                    (df['dropoff_latitude'] <= NEW_YORK_BOX_COORDS[1][0]) & \n",
    "                    (df['dropoff_longitude'] >= NEW_YORK_BOX_COORDS[0][1]) & \n",
    "                    (df['dropoff_longitude'] <= NEW_YORK_BOX_COORDS[1][1])]\n",
    "\n",
    "             # Normalize column names\n",
    "            df.columns = [col.lower().strip().replace(' ', '_') for col in df.columns]\n",
    "\n",
    "            # Drop unnecessary columns\n",
    "            df = df[['pickup_datetime',\n",
    "                     'dropoff_datetime',\n",
    "                     'trip_miles',\n",
    "                     'trip_time',\n",
    "                     'base_passenger_fare',\n",
    "                     'tolls',\n",
    "                     'bcf',\n",
    "                     'sales_tax',\n",
    "                     'congestion_surcharge',\n",
    "                     'airport_fee',\n",
    "                     'tips',\n",
    "                     'pickup_latitude',\n",
    "                     'pickup_longitude',\n",
    "                     'dropoff_latitude',\n",
    "                     'dropoff_longitude']]\n",
    "            \n",
    "            # convert datatype after filtering\n",
    "            df['pickup_datetime'] = pd.to_datetime(df['pickup_datetime'], errors='coerce')\n",
    "            df['dropoff_datetime'] = pd.to_datetime(df['dropoff_datetime'], errors='coerce')\n",
    "            df['trip_miles'] = df['trip_miles'].astype(float)\n",
    "            df['trip_time'] = df['trip_time'].astype(float)\n",
    "            df['base_passenger_fare'] = df['base_passenger_fare'].astype(float)\n",
    "            df['tolls'] = df['tolls'].astype(float)\n",
    "            df['bcf'] = df['bcf'].astype(float)\n",
    "            df['sales_tax'] = df['sales_tax'].astype(float)\n",
    "            df['congestion_surcharge'] = df['congestion_surcharge'].astype(float)\n",
    "            df['airport_fee'] = df['airport_fee'].fillna(0).astype(float)\n",
    "            df['tips'] = df['tips'].astype(float)\n",
    "            df['pickup_latitude'] = df['pickup_latitude'].astype(float)\n",
    "            df['pickup_longitude'] = df['pickup_longitude'].astype(float)\n",
    "            df['dropoff_latitude'] = df['dropoff_latitude'].astype(float)\n",
    "            df['dropoff_longitude'] = df['dropoff_longitude'].astype(float)\n",
    "\n",
    "            # calculate sample size and generate sampled_df\n",
    "            population_size = len(df)\n",
    "            sample_size = calculate_sample_size(population_size, confidence_level=0.95, margin_of_error=0.05)\n",
    "            sampled_df = df.sample(n=sample_size, random_state=42)\n",
    "\n",
    "            # Save cleaned file as intermediate Parquet\n",
    "            intermediate_file = os.path.join(intermediate_directory, os.path.basename(file_path))\n",
    "            sampled_df.to_parquet(intermediate_file, index=False)\n",
    "            print(f\"Saved intermediate cleaned data to: {intermediate_file}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {file_path}: {e}\")\n",
    "\n",
    "    # Combine all intermediate files into the final Parquet file\n",
    "    intermediate_files = glob.glob(f\"{intermediate_directory}/*.parquet\")\n",
    "    all_dataframes = []\n",
    "    \n",
    "    for intermediate_file in intermediate_files:\n",
    "        try:\n",
    "            print(f\"Loading intermediate file: {intermediate_file}\")\n",
    "            df = pd.read_parquet(intermediate_file)\n",
    "            all_dataframes.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading intermediate file {intermediate_file}: {e}\")\n",
    "\n",
    "\n",
    "    # Concatenate all cleaned intermediate files and save as one large file\n",
    "    if all_dataframes:\n",
    "        combined_df = pd.concat(all_dataframes, ignore_index=True)\n",
    "        combined_df.to_parquet(output_file, index=False)\n",
    "        print(f\"Saved combined data to: {output_file}\")\n",
    "    else:\n",
    "        print(\"No valid data to combine.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c690d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "uber_input_directory = f\"{dataset_directory}/fhvhv_tripdata\"\n",
    "uber_intermediate_directory = f\"{dataset_directory}/intermediate_cleaned_uber\"\n",
    "uber_output_file = f\"{dataset_directory}/combined_uber_data.parquet\"\n",
    "\n",
    "apply_cleaning_and_combine_uber(uber_input_directory, uber_intermediate_directory, uber_output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e307940",
   "metadata": {},
   "outputs": [],
   "source": [
    "uber_data = pd.read_parquet(uber_output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbe6c5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>dropoff_datetime</th>\n",
       "      <th>trip_miles</th>\n",
       "      <th>trip_time</th>\n",
       "      <th>base_passenger_fare</th>\n",
       "      <th>tolls</th>\n",
       "      <th>bcf</th>\n",
       "      <th>sales_tax</th>\n",
       "      <th>congestion_surcharge</th>\n",
       "      <th>airport_fee</th>\n",
       "      <th>tips</th>\n",
       "      <th>pickup_latitude</th>\n",
       "      <th>pickup_longitude</th>\n",
       "      <th>dropoff_latitude</th>\n",
       "      <th>dropoff_longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-03-10 07:37:01</td>\n",
       "      <td>2021-03-10 07:48:59</td>\n",
       "      <td>2.15</td>\n",
       "      <td>718.0</td>\n",
       "      <td>10.71</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.688721</td>\n",
       "      <td>-73.855767</td>\n",
       "      <td>40.694542</td>\n",
       "      <td>-73.830924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-03-18 08:21:36</td>\n",
       "      <td>2021-03-18 08:33:29</td>\n",
       "      <td>1.34</td>\n",
       "      <td>713.0</td>\n",
       "      <td>9.79</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>40.736823</td>\n",
       "      <td>-73.984052</td>\n",
       "      <td>40.753512</td>\n",
       "      <td>-73.988786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-03-17 07:48:16</td>\n",
       "      <td>2021-03-17 08:02:38</td>\n",
       "      <td>4.56</td>\n",
       "      <td>862.0</td>\n",
       "      <td>18.28</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.55</td>\n",
       "      <td>1.62</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.742671</td>\n",
       "      <td>-73.754622</td>\n",
       "      <td>40.783332</td>\n",
       "      <td>-73.785972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-03-14 10:52:12</td>\n",
       "      <td>2021-03-14 11:03:28</td>\n",
       "      <td>2.69</td>\n",
       "      <td>676.0</td>\n",
       "      <td>15.16</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.45</td>\n",
       "      <td>1.35</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.876512</td>\n",
       "      <td>-73.895620</td>\n",
       "      <td>40.882403</td>\n",
       "      <td>-73.910665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-03-28 11:50:29</td>\n",
       "      <td>2021-03-28 11:57:33</td>\n",
       "      <td>0.68</td>\n",
       "      <td>424.0</td>\n",
       "      <td>14.15</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.42</td>\n",
       "      <td>1.26</td>\n",
       "      <td>2.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.717772</td>\n",
       "      <td>-74.007880</td>\n",
       "      <td>40.717772</td>\n",
       "      <td>-74.007880</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      pickup_datetime    dropoff_datetime  trip_miles  trip_time  \\\n",
       "0 2021-03-10 07:37:01 2021-03-10 07:48:59        2.15      718.0   \n",
       "1 2021-03-18 08:21:36 2021-03-18 08:33:29        1.34      713.0   \n",
       "2 2021-03-17 07:48:16 2021-03-17 08:02:38        4.56      862.0   \n",
       "3 2021-03-14 10:52:12 2021-03-14 11:03:28        2.69      676.0   \n",
       "4 2021-03-28 11:50:29 2021-03-28 11:57:33        0.68      424.0   \n",
       "\n",
       "   base_passenger_fare  tolls   bcf  sales_tax  congestion_surcharge  \\\n",
       "0                10.71    0.0  0.32       0.95                  0.00   \n",
       "1                 9.79    0.0  0.00       0.00                  2.75   \n",
       "2                18.28    0.0  0.55       1.62                  0.00   \n",
       "3                15.16    0.0  0.45       1.35                  0.00   \n",
       "4                14.15    0.0  0.42       1.26                  2.75   \n",
       "\n",
       "   airport_fee  tips  pickup_latitude  pickup_longitude  dropoff_latitude  \\\n",
       "0          0.0   0.0        40.688721        -73.855767         40.694542   \n",
       "1          0.0   2.0        40.736823        -73.984052         40.753512   \n",
       "2          0.0   0.0        40.742671        -73.754622         40.783332   \n",
       "3          0.0   0.0        40.876512        -73.895620         40.882403   \n",
       "4          0.0   0.0        40.717772        -74.007880         40.717772   \n",
       "\n",
       "   dropoff_longitude  \n",
       "0         -73.830924  \n",
       "1         -73.988786  \n",
       "2         -73.785972  \n",
       "3         -73.910665  \n",
       "4         -74.007880  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uber_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53fecf0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 21560 entries, 0 to 21559\n",
      "Data columns (total 15 columns):\n",
      " #   Column                Non-Null Count  Dtype         \n",
      "---  ------                --------------  -----         \n",
      " 0   pickup_datetime       21560 non-null  datetime64[ns]\n",
      " 1   dropoff_datetime      21560 non-null  datetime64[ns]\n",
      " 2   trip_miles            21560 non-null  float64       \n",
      " 3   trip_time             21560 non-null  float64       \n",
      " 4   base_passenger_fare   21560 non-null  float64       \n",
      " 5   tolls                 21560 non-null  float64       \n",
      " 6   bcf                   21560 non-null  float64       \n",
      " 7   sales_tax             21560 non-null  float64       \n",
      " 8   congestion_surcharge  21560 non-null  float64       \n",
      " 9   airport_fee           21560 non-null  float64       \n",
      " 10  tips                  21560 non-null  float64       \n",
      " 11  pickup_latitude       21560 non-null  float64       \n",
      " 12  pickup_longitude      21560 non-null  float64       \n",
      " 13  dropoff_latitude      21560 non-null  float64       \n",
      " 14  dropoff_longitude     21560 non-null  float64       \n",
      "dtypes: datetime64[ns](2), float64(13)\n",
      "memory usage: 2.5 MB\n"
     ]
    }
   ],
   "source": [
    "uber_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a804b458",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>dropoff_datetime</th>\n",
       "      <th>trip_miles</th>\n",
       "      <th>trip_time</th>\n",
       "      <th>base_passenger_fare</th>\n",
       "      <th>tolls</th>\n",
       "      <th>bcf</th>\n",
       "      <th>sales_tax</th>\n",
       "      <th>congestion_surcharge</th>\n",
       "      <th>airport_fee</th>\n",
       "      <th>tips</th>\n",
       "      <th>pickup_latitude</th>\n",
       "      <th>pickup_longitude</th>\n",
       "      <th>dropoff_latitude</th>\n",
       "      <th>dropoff_longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>21560</td>\n",
       "      <td>21560</td>\n",
       "      <td>21560.000000</td>\n",
       "      <td>21560.000000</td>\n",
       "      <td>21560.000000</td>\n",
       "      <td>21560.000000</td>\n",
       "      <td>21560.000000</td>\n",
       "      <td>21560.000000</td>\n",
       "      <td>21560.000000</td>\n",
       "      <td>21560.000000</td>\n",
       "      <td>21560.000000</td>\n",
       "      <td>21560.000000</td>\n",
       "      <td>21560.000000</td>\n",
       "      <td>21560.000000</td>\n",
       "      <td>21560.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2022-05-02 01:18:57.684322816</td>\n",
       "      <td>2022-05-02 01:36:50.816744192</td>\n",
       "      <td>4.379178</td>\n",
       "      <td>1073.391141</td>\n",
       "      <td>21.124263</td>\n",
       "      <td>0.628066</td>\n",
       "      <td>0.616259</td>\n",
       "      <td>1.883285</td>\n",
       "      <td>1.048782</td>\n",
       "      <td>0.133291</td>\n",
       "      <td>0.792580</td>\n",
       "      <td>40.737920</td>\n",
       "      <td>-73.934695</td>\n",
       "      <td>40.737407</td>\n",
       "      <td>-73.934451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2020-01-01 00:21:55</td>\n",
       "      <td>2020-01-01 00:40:45</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>68.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>40.561994</td>\n",
       "      <td>-74.186419</td>\n",
       "      <td>40.561994</td>\n",
       "      <td>-74.174000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2021-03-01 02:25:26.750000128</td>\n",
       "      <td>2021-03-01 02:43:57.500000</td>\n",
       "      <td>1.550000</td>\n",
       "      <td>560.000000</td>\n",
       "      <td>10.560000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.290000</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>40.691201</td>\n",
       "      <td>-73.984196</td>\n",
       "      <td>40.691201</td>\n",
       "      <td>-73.984052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2022-04-30 22:29:27</td>\n",
       "      <td>2022-04-30 22:48:34.500000</td>\n",
       "      <td>2.820000</td>\n",
       "      <td>880.000000</td>\n",
       "      <td>16.590000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.470000</td>\n",
       "      <td>1.460000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>40.737698</td>\n",
       "      <td>-73.948789</td>\n",
       "      <td>40.737698</td>\n",
       "      <td>-73.947442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2023-07-01 00:01:51.249999872</td>\n",
       "      <td>2023-07-01 00:14:48.500000</td>\n",
       "      <td>5.570000</td>\n",
       "      <td>1378.000000</td>\n",
       "      <td>26.190000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.760000</td>\n",
       "      <td>2.360000</td>\n",
       "      <td>2.750000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>40.774375</td>\n",
       "      <td>-73.899735</td>\n",
       "      <td>40.775932</td>\n",
       "      <td>-73.898956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2024-08-31 23:07:13</td>\n",
       "      <td>2024-08-31 23:19:20</td>\n",
       "      <td>42.780000</td>\n",
       "      <td>8862.000000</td>\n",
       "      <td>213.740000</td>\n",
       "      <td>46.210000</td>\n",
       "      <td>6.140000</td>\n",
       "      <td>19.810000</td>\n",
       "      <td>2.750000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>40.899529</td>\n",
       "      <td>-73.726656</td>\n",
       "      <td>40.899529</td>\n",
       "      <td>-73.726656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.285147</td>\n",
       "      <td>740.388956</td>\n",
       "      <td>15.781332</td>\n",
       "      <td>2.525454</td>\n",
       "      <td>0.500672</td>\n",
       "      <td>1.435071</td>\n",
       "      <td>1.330371</td>\n",
       "      <td>0.566746</td>\n",
       "      <td>2.399464</td>\n",
       "      <td>0.068442</td>\n",
       "      <td>0.064763</td>\n",
       "      <td>0.068879</td>\n",
       "      <td>0.067801</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     pickup_datetime               dropoff_datetime  \\\n",
       "count                          21560                          21560   \n",
       "mean   2022-05-02 01:18:57.684322816  2022-05-02 01:36:50.816744192   \n",
       "min              2020-01-01 00:21:55            2020-01-01 00:40:45   \n",
       "25%    2021-03-01 02:25:26.750000128     2021-03-01 02:43:57.500000   \n",
       "50%              2022-04-30 22:29:27     2022-04-30 22:48:34.500000   \n",
       "75%    2023-07-01 00:01:51.249999872     2023-07-01 00:14:48.500000   \n",
       "max              2024-08-31 23:07:13            2024-08-31 23:19:20   \n",
       "std                              NaN                            NaN   \n",
       "\n",
       "         trip_miles     trip_time  base_passenger_fare         tolls  \\\n",
       "count  21560.000000  21560.000000         21560.000000  21560.000000   \n",
       "mean       4.379178   1073.391141            21.124263      0.628066   \n",
       "min        0.040000     68.000000             0.000000      0.000000   \n",
       "25%        1.550000    560.000000            10.560000      0.000000   \n",
       "50%        2.820000    880.000000            16.590000      0.000000   \n",
       "75%        5.570000   1378.000000            26.190000      0.000000   \n",
       "max       42.780000   8862.000000           213.740000     46.210000   \n",
       "std        4.285147    740.388956            15.781332      2.525454   \n",
       "\n",
       "                bcf     sales_tax  congestion_surcharge   airport_fee  \\\n",
       "count  21560.000000  21560.000000          21560.000000  21560.000000   \n",
       "mean       0.616259      1.883285              1.048782      0.133291   \n",
       "min        0.000000      0.000000              0.000000      0.000000   \n",
       "25%        0.290000      0.920000              0.000000      0.000000   \n",
       "50%        0.470000      1.460000              0.000000      0.000000   \n",
       "75%        0.760000      2.360000              2.750000      0.000000   \n",
       "max        6.140000     19.810000              2.750000      5.000000   \n",
       "std        0.500672      1.435071              1.330371      0.566746   \n",
       "\n",
       "               tips  pickup_latitude  pickup_longitude  dropoff_latitude  \\\n",
       "count  21560.000000     21560.000000      21560.000000      21560.000000   \n",
       "mean       0.792580        40.737920        -73.934695         40.737407   \n",
       "min        0.000000        40.561994        -74.186419         40.561994   \n",
       "25%        0.000000        40.691201        -73.984196         40.691201   \n",
       "50%        0.000000        40.737698        -73.948789         40.737698   \n",
       "75%        0.000000        40.774375        -73.899735         40.775932   \n",
       "max       50.000000        40.899529        -73.726656         40.899529   \n",
       "std        2.399464         0.068442          0.064763          0.068879   \n",
       "\n",
       "       dropoff_longitude  \n",
       "count       21560.000000  \n",
       "mean          -73.934451  \n",
       "min           -74.174000  \n",
       "25%           -73.984052  \n",
       "50%           -73.947442  \n",
       "75%           -73.898956  \n",
       "max           -73.726656  \n",
       "std             0.067801  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uber_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9588cf84",
   "metadata": {},
   "source": [
    "\n",
    "### Processing Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900e5949",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_weather_csvs(directory):\n",
    "    # Create the path pattern for all CSV files\n",
    "    csv_pattern = os.path.join(directory, \"*.csv\")\n",
    "    \n",
    "    # Use glob to get all the CSV files\n",
    "    csv_files = glob.glob(csv_pattern)\n",
    "    \n",
    "    # Return the list of CSV files\n",
    "    return csv_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599cdc25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_month_weather_data_hourly(csv_file):\n",
    "    \"\"\"\n",
    "    Reads a CSV file containing hourly weather data and preprocesses it.\n",
    "\n",
    "    Preprocessing Steps:\n",
    "    1. Retains only relevant columns: 'DATE', 'HourlyPrecipitation', 'HourlyWindSpeed', 'DailyAverageWindSpeed'.\n",
    "    2. Converts 'DATE' column to datetime format.\n",
    "    3. Cleans 'HourlyPrecipitation' by:\n",
    "       - Replacing 'T' (trace precipitation) with 0.\n",
    "       - Filling missing values with 0.\n",
    "    4. Cleans 'HourlyWindSpeed' by:\n",
    "       - Using 'DailyAverageWindSpeed' if 'HourlyWindSpeed' is blank.\n",
    "       - If both are blank, fills 'HourlyWindSpeed' with 0.\n",
    "    5. Drops 'DailyAverageWindSpeed' column at the end.\n",
    "\n",
    "    Args:\n",
    "        csv_file (str): Path to the CSV file containing the weather data.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Cleaned DataFrame with only the relevant columns and filled missing values.\n",
    "    \"\"\"\n",
    "\n",
    "    hourly_df = pd.read_csv(csv_file)\n",
    "\n",
    "    # keep only the relevant columns\n",
    "    required_columns = ['DATE', 'HourlyPrecipitation', 'HourlyWindSpeed', 'DailyAverageWindSpeed']\n",
    "    hourly_df = hourly_df[required_columns]\n",
    "\n",
    "    # Convert 'DATE' column to datetime\n",
    "    hourly_df['DATE'] = pd.to_datetime(hourly_df['DATE'], errors='coerce')\n",
    "\n",
    "    # Handle HourlyPrecipitation: replace 'T' with 0 and fill missing values\n",
    "    hourly_df['HourlyPrecipitation'] = hourly_df['HourlyPrecipitation'].replace({'T': 0}).fillna(0)\n",
    "\n",
    "    # Handle HourlyWindSpeed using DailyAverageWindSpeed or defaulting to 0\n",
    "    hourly_df['HourlyWindSpeed'] = pd.to_numeric(hourly_df['HourlyWindSpeed'], errors='coerce')\n",
    "    hourly_df['DailyAverageWindSpeed'] = pd.to_numeric(hourly_df['DailyAverageWindSpeed'], errors='coerce')\n",
    "    hourly_df['HourlyWindSpeed'] = hourly_df.apply(\n",
    "        lambda row: row['DailyAverageWindSpeed'] if pd.isna(row['HourlyWindSpeed']) else row['HourlyWindSpeed'], axis=1\n",
    "    ).fillna(0)\n",
    "\n",
    "    # Drop DailyAverageWindSpeed column\n",
    "    hourly_df.drop(columns=['DailyAverageWindSpeed'], inplace=True)\n",
    "\n",
    "    # Normalize column names\n",
    "    hourly_df.columns = [col.lower().strip().replace(' ', '_') for col in hourly_df.columns]\n",
    "\n",
    "    return hourly_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8409d2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_month_weather_data_daily(csv_file):\n",
    "    \"\"\"\n",
    "    Preprocess a CSV file containing daily weather data.\n",
    "\n",
    "    This function cleans and processes the weather data, retaining only relevant columns,\n",
    "    handling missing values, and converting specific fields (e.g., sunrise and sunset times)\n",
    "    into a usable format.\n",
    "\n",
    "    Args:\n",
    "        csv_file (str): Path to the CSV file containing daily weather data.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A cleaned DataFrame with the following columns:\n",
    "            - DATE (datetime): The date of the weather record.\n",
    "            - DailyPrecipitation (float): The daily precipitation amount.\n",
    "            - DailyAverageWindSpeed (float): The average wind speed for the day.\n",
    "            - DailySnowfall (float): The daily snowfall amount.\n",
    "            - Sunrise (time): The time of sunrise, formatted as HH:MM.\n",
    "            - Sunset (time): The time of sunset, formatted as HH:MM.\n",
    "    \"\"\"\n",
    "    # Read the CSV file with low_memory=False to avoid mixed types\n",
    "    daily_df = pd.read_csv(csv_file, low_memory=False)\n",
    "\n",
    "    # Keep only the relevant columns\n",
    "    relevant_columns = [\n",
    "        'DATE', 'DailyPrecipitation', 'DailyAverageWindSpeed', \n",
    "        'DailySnowfall', 'Sunrise', 'Sunset', \n",
    "    ]\n",
    "    daily_df = daily_df[relevant_columns]\n",
    "\n",
    "    # Convert 'DATE' column to datetime\n",
    "    daily_df['DATE'] = pd.to_datetime(daily_df['DATE'], errors='coerce')\n",
    "\n",
    "    # convert 'T' to 0\n",
    "    daily_df.replace(\"T\", 0, inplace=True)\n",
    "    \n",
    "    # Ensure numeric columns are converted properly\n",
    "    numeric_columns = ['DailyPrecipitation', 'DailyAverageWindSpeed', 'DailySnowfall']\n",
    "    for col in numeric_columns:\n",
    "        daily_df[col] = pd.to_numeric(daily_df[col], errors='coerce')\n",
    "\n",
    "    # Drop rows where the columns values are NaN\n",
    "    daily_df = daily_df.dropna(subset=['DailyPrecipitation', 'DailyAverageWindSpeed', 'DailySnowfall', 'Sunrise', 'Sunset'], how=\"all\")\n",
    "\n",
    "\n",
    "    # Convert Sunrise and Sunset to time format\n",
    "    if 'Sunrise' in daily_df.columns:\n",
    "        daily_df['Sunrise'] = pd.to_datetime(daily_df['Sunrise'], format='%H%M', errors='coerce').dt.time\n",
    "    if 'Sunset' in daily_df.columns:\n",
    "        daily_df['Sunset'] = pd.to_datetime(daily_df['Sunset'], format='%H%M', errors='coerce').dt.time\n",
    "\n",
    "\n",
    "    return daily_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d92aa8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_weather_data():\n",
    "    \"\"\"\n",
    "    Loads and cleans weather data from multiple CSV files.\n",
    "    Separately processes hourly and daily data from the same files.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame, pd.DataFrame: Combined hourly and daily weather data.\n",
    "    \"\"\"\n",
    "    # Get the list of all weather CSV files\n",
    "    weather_csv_files = get_all_weather_csvs(WEATHER_CSV_DIR)\n",
    "\n",
    "    hourly_dataframes = []\n",
    "    daily_dataframes = []\n",
    "\n",
    "    for csv_file in weather_csv_files:\n",
    "        try:\n",
    "            hourly_dataframe = clean_month_weather_data_hourly(csv_file)\n",
    "            hourly_dataframes.append(hourly_dataframe)\n",
    "\n",
    "            daily_dataframe = clean_month_weather_data_daily(csv_file)\n",
    "            daily_dataframes.append(daily_dataframe)\n",
    "        except ValueError as e:\n",
    "            print(f\"Skipping file {csv_file} due to error: {e}\")\n",
    "\n",
    "    hourly_weather_data = pd.concat(hourly_dataframes, ignore_index=True) if hourly_dataframes else pd.DataFrame()\n",
    "    daily_weather_data = pd.concat(daily_dataframes, ignore_index=True) if daily_dataframes else pd.DataFrame()\n",
    "\n",
    "    return hourly_weather_data, daily_weather_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47b9dac",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'WEATHER_CSV_DIR' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m hourly_weather_data, daily_weather_data \u001b[38;5;241m=\u001b[39m load_and_clean_weather_data()\n",
      "Cell \u001b[1;32mIn[18], line 10\u001b[0m, in \u001b[0;36mload_and_clean_weather_data\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03mLoads and cleans weather data from multiple CSV files.\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03mSeparately processes hourly and daily data from the same files.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;124;03m    pd.DataFrame, pd.DataFrame: Combined hourly and daily weather data.\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Get the list of all weather CSV files\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m weather_csv_files \u001b[38;5;241m=\u001b[39m get_all_weather_csvs(WEATHER_CSV_DIR)\n\u001b[0;32m     12\u001b[0m hourly_dataframes \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     13\u001b[0m daily_dataframes \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[1;31mNameError\u001b[0m: name 'WEATHER_CSV_DIR' is not defined"
     ]
    }
   ],
   "source": [
    "hourly_weather_data, daily_weather_data = load_and_clean_weather_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c833786d",
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_weather_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09d47a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_weather_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b7ed04",
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_weather_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e35d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_weather_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e268dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_weather_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9961c905",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_weather_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede37fb6",
   "metadata": {},
   "source": [
    "## Part 2: Storing Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d6cb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = db.create_engine(DATABASE_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b984a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if using SQL (as opposed to SQLAlchemy), define the commands \n",
    "# to create your 4 tables/dataframes\n",
    "HOURLY_WEATHER_SCHEMA = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS hourly_weather (\n",
    "    id INTEGER PRIMARY KEY,\n",
    "    DATE DATETIME,\n",
    "    hourly_precipitation FLOAT,\n",
    "    hourly_wind_speed FLOAT\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "DAILY_WEATHER_SCHEMA = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS daily_weather (\n",
    "    id INTEGER PRIMARY KEY,\n",
    "    date DATETIME,\n",
    "    daily_precipitation FLOAT,\n",
    "    daily_wind_speed FLOAT,\n",
    "    snowfall FLOAT\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "TAXI_TRIPS_SCHEMA = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS taxi_trips (\n",
    "    trip_id INTEGER PRIMARY KEY,\n",
    "    tpep_pickup_datetime DATETIME,\n",
    "    tpep_dropoff_datetime DATETIME,\n",
    "    trip_distance FLOAT,\n",
    "    fare_amount FLOAT,\n",
    "    tip_amount FLOAT,\n",
    "    total_amount FLOAT,\n",
    "    pickup_latitude FLOAT,\n",
    "    pickup_longitude FLOAT,\n",
    "    dropoff_latitude FLOAT,\n",
    "    dropoff_longitude FLOAT\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "UBER_TRIPS_SCHEMA = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS uber_trips (\n",
    "    id INTEGER PRIMARY KEY,\n",
    "    pickup_datetime DATETIME,\n",
    "    dropoff_datetime DATETIME,\n",
    "    pickup_latitude FLOAT,\n",
    "    pickup_longitude FLOAT,\n",
    "    dropoff_latitude FLOAT,\n",
    "    dropoff_longitude FLOAT,\n",
    "    trip_miles FLOAT,\n",
    "    fare_amount FLOAT,\n",
    "    tolls_amount FLOAT,\n",
    "    bcf FLOAT,\n",
    "    sales_tax FLOAT,\n",
    "    tip_amount FLOAT,\n",
    "    airport_fee FLOAT,\n",
    "    congestion_surcharge FLOAT\n",
    ");\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6313ed3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create that required schema.sql file\n",
    "\n",
    "with open(DATABASE_SCHEMA_FILE, \"w\") as f:\n",
    "    f.write(HOURLY_WEATHER_SCHEMA)\n",
    "    f.write(DAILY_WEATHER_SCHEMA)\n",
    "    f.write(TAXI_TRIPS_SCHEMA)\n",
    "    f.write(UBER_TRIPS_SCHEMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc81179",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the tables with the schema files\n",
    "with engine.connect() as connection:\n",
    "    connection.execute(HOURLY_WEATHER_SCHEMA)\n",
    "    connection.execute(DAILY_WEATHER_SCHEMA)\n",
    "    connection.execute(TAXI_TRIPS_SCHEMA)\n",
    "    connection.execute(UBER_TRIPS_SCHEMA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ccc429",
   "metadata": {},
   "source": [
    "### Add Data to Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c411599b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# map tables to dataframes and writes them using to_sql\n",
    "def write_dataframes_to_table(table_to_df_dict):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        table_to_df_dict (dict): A dictionary where:\n",
    "            - Keys are table names (str) to be written to the database.\n",
    "            - Values are pandas DataFrames containing the data to be written.\n",
    "\n",
    "    Returns:\n",
    "        None: This function writes data to the database and does not return any value.\n",
    "    \"\"\"\n",
    "    for table, dataframe in table_to_df_dict.items():\n",
    "        dataframe.to_sql(table, engine, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc57530",
   "metadata": {},
   "outputs": [],
   "source": [
    "# map dataframes to tables\n",
    "\n",
    "map_table_name_to_dataframe = {\n",
    "    \"taxi_trips\": taxi_data,\n",
    "    \"uber_trips\": uber_data,\n",
    "    \"hourly_weather\": hourly_weather_data,\n",
    "    \"daily_weather\": daily_weather_data\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5190f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write dataframes to tables\n",
    "write_dataframes_to_table(map_table_name_to_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2855f1a3",
   "metadata": {},
   "source": [
    "## Part 3: Understanding the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a562b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to write the queries to file\n",
    "def write_query_to_file(query, outfile):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        query (str): The SQL query to be written to the file.\n",
    "        outfile (str): The name of the file to save the query in.\n",
    "\n",
    "    Returns:\n",
    "        None: This function writes the query to a file and does not return any value.\n",
    "\n",
    "    \"\"\"\n",
    "    filepath = os.path.join(QUERY_DIRECTORY, outfile)\n",
    "    with open(filepath, \"w\") as f:\n",
    "        f.write(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c56e67",
   "metadata": {},
   "source": [
    "### Query 1: Whatâ€™s the most popular hour to take a taxi?\n",
    "Q: For 01-2020 through 08-2024, show the popularity of Yellow Taxi rides for each hour of the day.<br>\n",
    "A: 18:00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9bddfeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_1_FILENAME = \"popular_taxi_hour.sql\"\n",
    "\n",
    "QUERY_1 = \"\"\"\n",
    "SELECT \n",
    "    strftime('%H', tpep_pickup_datetime) AS hour,\n",
    "    COUNT(*) AS ride_count\n",
    "FROM taxi_trips\n",
    "WHERE tpep_pickup_datetime BETWEEN '2020-01-01' AND '2024-08-31'\n",
    "GROUP BY hour\n",
    "ORDER BY ride_count DESC;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270b3c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "query1 = pd.read_sql(QUERY_1, con=engine)\n",
    "print(query1.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f79b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_1, QUERY_1_FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efedabad",
   "metadata": {},
   "source": [
    "### Query 2: Whatâ€™s the most popular day of the week to take an Uber?\n",
    "Q: For the same time frame, show the popularity of Uber rides for each day of the week.<br>\n",
    "A: Saturday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8baceae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_2_FILENAME = \"popular_uber_day.sql\"\n",
    "\n",
    "QUERY_2 = \"\"\"\n",
    "SELECT \n",
    "    strftime('%w', pickup_datetime) AS day_of_week,\n",
    "    COUNT(*) AS ride_count\n",
    "FROM uber_trips\n",
    "WHERE pickup_datetime BETWEEN '2020-01-01' AND '2024-08-31'\n",
    "GROUP BY day_of_week\n",
    "ORDER BY ride_count DESC;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5995c856",
   "metadata": {},
   "outputs": [],
   "source": [
    "query2 = pd.read_sql(QUERY_2, con=engine)\n",
    "print(query2.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db6b274",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_2, QUERY_2_FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1465abed",
   "metadata": {},
   "source": [
    "### Query 3: Whatâ€™s the 95% percentile of trip distance in January 2024?\n",
    "Q: What is the 95% percentile of distance traveled for all hired rides trips during January 2024? <br>\n",
    "A: 14.06 miles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2cbc64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_3_FILENAME = \"95_percentile_Jan2024_trip_distance.sql\"\n",
    "\n",
    "QUERY_3 = \"\"\"\n",
    "SELECT trip_distance\n",
    "FROM (\n",
    "    SELECT trip_distance\n",
    "    FROM (\n",
    "        SELECT trip_distance\n",
    "        FROM taxi_trips\n",
    "        WHERE tpep_pickup_datetime BETWEEN '2024-01-01' AND '2024-01-31'\n",
    "        UNION ALL\n",
    "        SELECT trip_miles AS trip_distance\n",
    "        FROM uber_trips\n",
    "        WHERE pickup_datetime BETWEEN '2024-01-01' AND '2024-01-31'\n",
    "    ) AS combined_trips\n",
    "    ORDER BY trip_distance\n",
    ") AS sorted_trips\n",
    "LIMIT 1 OFFSET (\n",
    "    SELECT CAST(COUNT(*) * 0.95 AS INTEGER) - 1\n",
    "    FROM (\n",
    "        SELECT trip_distance\n",
    "        FROM taxi_trips\n",
    "        WHERE tpep_pickup_datetime BETWEEN '2024-01-01' AND '2024-01-31'\n",
    "        UNION ALL\n",
    "        SELECT trip_miles AS trip_distance\n",
    "        FROM uber_trips\n",
    "        WHERE pickup_datetime BETWEEN '2024-01-01' AND '2024-01-31'\n",
    "    ) AS combined_trips_for_count\n",
    ");\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12aa3d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "query3 = pd.read_sql(QUERY_3, con=engine)\n",
    "print(query3.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e882c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_3, QUERY_3_FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20191ec9",
   "metadata": {},
   "source": [
    "### Query 4: What was the weather like for the busiest days in 2023?\n",
    "What were the top 10 days with the highest number of all hired rides for 2023, and for each day, what was the average distance, average precipitation amount, and average wind speed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad93bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_4_FILENAME = \"busiest_day_weather_2023.sql\"\n",
    "\n",
    "QUERY_4 = \"\"\"\n",
    "SELECT t.ride_date, \n",
    "       t.total_rides, \n",
    "       t.avg_distance, \n",
    "       w.DailyPrecipitation AS avg_precipitation, \n",
    "       w.DailyAverageWindSpeed AS avg_wind_speed\n",
    "FROM (\n",
    "    SELECT strftime('%Y-%m-%d', pickup_datetime) AS ride_date,\n",
    "           COUNT(*) AS total_rides,\n",
    "           AVG(trip_distance) AS avg_distance\n",
    "    FROM (\n",
    "        SELECT tpep_pickup_datetime AS pickup_datetime, trip_distance\n",
    "        FROM taxi_trips\n",
    "        WHERE tpep_pickup_datetime BETWEEN '2023-01-01' AND '2023-12-31'\n",
    "        UNION ALL\n",
    "        SELECT pickup_datetime, trip_miles AS trip_distance\n",
    "        FROM uber_trips\n",
    "        WHERE pickup_datetime BETWEEN '2023-01-01' AND '2023-12-31'\n",
    "    ) AS combined_rides\n",
    "    GROUP BY ride_date\n",
    ") AS t\n",
    "LEFT JOIN (\n",
    "    SELECT DATE(DATE) AS ride_date,\n",
    "           DailyPrecipitation,\n",
    "           DailyAverageWindSpeed\n",
    "    FROM daily_weather\n",
    ") AS w ON t.ride_date = w.ride_date\n",
    "ORDER BY t.total_rides DESC\n",
    "LIMIT 10;\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c48dee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "query4 = pd.read_sql(QUERY_4, con=engine)\n",
    "print(query4.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f3d215",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_4, QUERY_4_FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e7cca7",
   "metadata": {},
   "source": [
    "### Query 5: How many rides were hired during snow days?\n",
    "Which 10 days in between January 2020 and August 2024 (inclusive) had the most snow, and how many hired trips were made on those days?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8948ee82",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_5_FILENAME = \"rides_count_snowday.sql\"\n",
    "\n",
    "QUERY_5 = \"\"\"\n",
    "SELECT t.ride_date, \n",
    "       t.total_rides, \n",
    "       w.DailySnowfall\n",
    "FROM(\n",
    "    SELECT strftime('%Y-%m-%d', pickup_datetime) AS ride_date,\n",
    "           COUNT(*) AS total_rides\n",
    "    FROM (\n",
    "        SELECT tpep_pickup_datetime AS pickup_datetime\n",
    "        FROM taxi_trips\n",
    "        WHERE tpep_pickup_datetime BETWEEN '2020-01-01' AND '2024-8-31'\n",
    "        UNION ALL\n",
    "        SELECT pickup_datetime\n",
    "        FROM uber_trips\n",
    "        WHERE pickup_datetime BETWEEN '2020-01-01' AND '2024-8-31'\n",
    "    ) AS combined_rides\n",
    "    GROUP BY ride_date\n",
    ") AS t\n",
    "LEFT JOIN (\n",
    "    SELECT DATE(DATE) AS ride_date,\n",
    "           DailySnowfall\n",
    "    FROM daily_weather\n",
    ") AS w ON t.ride_date = w.ride_date\n",
    "ORDER BY w.DailySnowfall DESC\n",
    "LIMIT 10;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c8657c",
   "metadata": {},
   "outputs": [],
   "source": [
    "query5 = pd.read_sql(QUERY_5, con=engine)\n",
    "print(query5.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7667f8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_5, QUERY_5_FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49a493f",
   "metadata": {},
   "source": [
    "### Query 6: \n",
    "Tropical Storm Ophelia (September 28-30, 2023) set a new daily rainfall record in NYC with 8.05 inches of rain measured, causing flooding across all of the city. During Ophelia, plus 3 days leading up to it and 3 days after it, how many trips were taken each hour, and for each hour, how much precipitation did NYC receive, and what was the sustained wind speed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1889b9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_6_FILENAME = \"Ophelia_rides.sql\"\n",
    "\n",
    "QUERY_6 = \"\"\"\n",
    "SELECT \n",
    "    t1.hour AS date_hour,\n",
    "    COALESCE(t2.total_rides, 0) AS total_rides,\n",
    "    COALESCE(t3.total_precipitation, 0.0) AS total_precipitation,\n",
    "    COALESCE(t3.sus_wind_speed, 0.0) AS sus_wind_speed\n",
    "FROM (\n",
    "    SELECT DISTINCT STRFTIME('%Y-%m-%d %H:00:00', DATE) AS hour\n",
    "    FROM hourly_weather\n",
    "    WHERE DATE BETWEEN '2023-09-25' AND '2023-10-03'\n",
    "    UNION\n",
    "    SELECT DISTINCT STRFTIME('%Y-%m-%d %H:00:00', tpep_pickup_datetime) AS hour\n",
    "    FROM taxi_trips\n",
    "    WHERE tpep_pickup_datetime BETWEEN '2023-09-25' AND '2023-10-03'\n",
    "    UNION\n",
    "    SELECT DISTINCT STRFTIME('%Y-%m-%d %H:00:00', pickup_datetime) AS hour\n",
    "    FROM uber_trips\n",
    "    WHERE pickup_datetime BETWEEN '2023-09-25' AND '2023-10-03'\n",
    ") AS t1\n",
    "LEFT JOIN (\n",
    "    SELECT \n",
    "        hour,\n",
    "        COUNT(*) AS total_rides\n",
    "    FROM (\n",
    "        SELECT \n",
    "            STRFTIME('%Y-%m-%d %H:00:00', tpep_pickup_datetime) AS hour\n",
    "        FROM taxi_trips\n",
    "        WHERE tpep_pickup_datetime BETWEEN '2023-09-25' AND '2023-10-03'\n",
    "        UNION ALL\n",
    "        SELECT \n",
    "            STRFTIME('%Y-%m-%d %H:00:00', pickup_datetime) AS hour\n",
    "        FROM uber_trips\n",
    "        WHERE pickup_datetime BETWEEN '2023-09-25' AND '2023-10-03'\n",
    "    ) AS all_rides\n",
    "    GROUP BY hour\n",
    ") AS t2 ON t1.hour = t2.hour\n",
    "LEFT JOIN (\n",
    "    SELECT \n",
    "        STRFTIME('%Y-%m-%d %H:00:00', date) AS hour,\n",
    "        SUM(HourlyPrecipitation) AS total_precipitation,\n",
    "        AVG(HourlyWindSpeed) AS sus_wind_speed\n",
    "    FROM hourly_weather\n",
    "    WHERE DATE BETWEEN '2023-09-25' AND '2023-10-03'\n",
    "    GROUP BY hour\n",
    ") AS t3 ON t1.hour = t3.hour\n",
    "ORDER BY t1.hour;\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515ca10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "query6 = pd.read_sql(QUERY_6, con=engine)\n",
    "print(query6.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03271bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_6, QUERY_6_FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de7b90e",
   "metadata": {},
   "source": [
    "### Part 4: Visualizing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30bac02",
   "metadata": {},
   "source": [
    "Visualization 1: Whatâ€™s the most popular hour to take a taxi?\n",
    "18:00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f623a98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a bar chart to show number of the rides by hour of the day\n",
    "\n",
    "def popular_taxi_hour(dataframe):\n",
    "\n",
    "    # Define the figure and axes for the plot\n",
    "    figure, axes = plt.subplots(figsize=(10, 7))\n",
    "    \n",
    "    # sort dataframe by ride_count\n",
    "    dataframe = dataframe.sort_values(by='ride_count')\n",
    "    \n",
    "    # Bar plot for visualizing ride counts by hour\n",
    "    hours = dataframe['hour']\n",
    "    ride_counts = dataframe['ride_count']\n",
    "    axes.bar(hours, ride_counts)\n",
    "    \n",
    "    # Adding labels, title, and styling\n",
    "    axes.set_title(\"Most Popular Hour to Take a Taxi\", fontsize=16)\n",
    "    axes.set_xlabel(\"Hour of the Day\", fontsize=14)\n",
    "    axes.set_ylabel(\"Number of Rides\", fontsize=14)\n",
    "    axes.set_xticks(range(len(hours)))  \n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad0f225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to call query 1 to get data\n",
    "\n",
    "def get_data_for_popular_taxi_hour():\n",
    "    with engine.connect() as con:\n",
    "        results = con.execute(db.text(QUERY_1)).fetchall()\n",
    "    return pd.DataFrame(results, columns=[\"hour\", \"ride_count\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29401927",
   "metadata": {},
   "outputs": [],
   "source": [
    "# execute query function and plot\n",
    "\n",
    "popular_taxi_hour_dataframe = get_data_for_popular_taxi_hour()\n",
    "popular_taxi_hour(popular_taxi_hour_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced66a5f",
   "metadata": {},
   "source": [
    "### Visualization 2: Whatâ€™s the most popular month to hire a cab or Uber?\n",
    "August"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a05bee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a plot to show average distance travelled by month\n",
    "\n",
    "def most_popular_month(dataframe):\n",
    "    \n",
    "    # Define the figure and axes for the plot\n",
    "    figure, axes = plt.subplots(figsize=(10, 7))\n",
    "    \n",
    "    # order by month\n",
    "    dataframe[\"month\"] = dataframe[\"month\"].astype(int)\n",
    "    dataframe = dataframe.sort_values(by=\"month\")\n",
    "    \n",
    "    # define values required for the chart - x and y axis and ci\n",
    "    months = dataframe[\"month\"]\n",
    "    avg_distances = dataframe[\"avg_distance\"]\n",
    "    ci_lower = dataframe[\"ci_lower\"]\n",
    "    ci_upper = dataframe[\"ci_upper\"]\n",
    "    \n",
    "    # Line plot with confidence interval\n",
    "    axes.plot(months, avg_distances, label=\"Average Distance\", marker='o')\n",
    "    axes.fill_between(months, ci_lower, ci_upper, alpha=0.2, label=\"90% Confidence Interval\")\n",
    "    \n",
    "    # Adding labels, title, and styling\n",
    "    axes.set_title(\"Average Distance Traveled per Month (2020-2024)\", fontsize=16)\n",
    "    axes.set_xlabel(\"Month\", fontsize=14)\n",
    "    axes.set_ylabel(\"Average Distance (miles)\", fontsize=14)\n",
    "    axes.set_xticks(range(len(months)))\n",
    "    axes.legend(fontsize=8)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320d33c4",
   "metadata": {},
   "source": [
    "# Combine taxi and uber trips by month. Pull average distance by month from the combined rides\n",
    "\n",
    "def get_data_for_most_popular_month():\n",
    "    \n",
    "    QUERY_2 = \"\"\"\n",
    "    WITH CombinedRides AS (\n",
    "        SELECT \n",
    "            strftime('%m', tpep_pickup_datetime) AS month,\n",
    "            trip_distance AS distance\n",
    "        FROM taxi_trips\n",
    "        WHERE tpep_pickup_datetime BETWEEN '2020-01-01 00:00:00' AND '2024-08-31 23:59:59'\n",
    "        \n",
    "        UNION ALL\n",
    "        \n",
    "        SELECT \n",
    "            strftime('%m', pickup_datetime) AS month,\n",
    "            trip_miles AS distance\n",
    "        FROM uber_trips\n",
    "        WHERE pickup_datetime BETWEEN '2020-01-01 00:00:00' AND '2024-08-31 23:59:59'\n",
    "    )\n",
    "    SELECT \n",
    "        month,\n",
    "        AVG(distance) AS avg_distance,\n",
    "        COUNT(distance) AS count_distance\n",
    "    FROM CombinedRides\n",
    "    GROUP BY month\n",
    "    ORDER BY month;\n",
    "    \"\"\"\n",
    "    \n",
    "    with engine.connect() as con:\n",
    "        results = con.execute(db.text(QUERY_2)).fetchall()\n",
    "    return pd.DataFrame(results, columns=[\"month\", \"avg_distance\", \"count_distance\"])\n",
    "\n",
    "\n",
    "# function to calculate confidence interval\n",
    "def calculate_confidence_intervals(dataframe):\n",
    "    # Assume a 90% confidence interval (Z = 1.645)\n",
    "    Z = 1.645\n",
    "    dataframe[\"ci_lower\"] = dataframe[\"avg_distance\"] - Z * (dataframe[\"avg_distance\"] / dataframe[\"count_distance\"]**0.5)\n",
    "    dataframe[\"ci_upper\"] = dataframe[\"avg_distance\"] + Z * (dataframe[\"avg_distance\"] / dataframe[\"count_distance\"]**0.5)\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85aeab2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# execute query function and plot\n",
    "data_for_most_popular_month = get_data_for_most_popular_month()\n",
    "data_for_most_popular_month = calculate_confidence_intervals(data_for_most_popular_month)\n",
    "most_popular_month(data_for_most_popular_month)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1433028e",
   "metadata": {},
   "source": [
    "### Visualization 3 - Which day of the week is most popular for taking a ride to an NYC-based airport?\n",
    "Sunday\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbc8dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a plot to show number of rides by day of the week\n",
    "\n",
    "def plot_popular_day_to_airport(dataframe):\n",
    "    \n",
    "    # Define the figure and axes for the plot\n",
    "    figure, axes = plt.subplots(figsize=(10, 7))  \n",
    "    \n",
    "    # Define mapping of days of the week for readability\n",
    "    day_mapping = {\n",
    "        \"0\": \"Sunday\",\n",
    "        \"1\": \"Monday\",\n",
    "        \"2\": \"Tuesday\",\n",
    "        \"3\": \"Wednesday\",\n",
    "        \"4\": \"Thursday\",\n",
    "        \"5\": \"Friday\",\n",
    "        \"6\": \"Saturday\"\n",
    "    }\n",
    "    \n",
    "    # Extract airport names and their respective ride counts\n",
    "    airports = [\"LGA\", \"JFK\", \"EWR\"]\n",
    "    for airport in airports:\n",
    "        subset = dataframe[dataframe['airport'] == airport]\n",
    "        # Sort by day_of_week \n",
    "        subset = subset.sort_values(by='day_of_week')\n",
    "        days = subset['day_of_week'].map(day_mapping)\n",
    "        ride_counts = subset['ride_count']\n",
    "        \n",
    "        # Plot for each airport\n",
    "        axes.plot(\n",
    "            days,\n",
    "            ride_counts,\n",
    "            label=f\"{airport} Airport\",\n",
    "            marker='o'\n",
    "        )\n",
    "\n",
    "    # Adding labels, title, and styling\n",
    "    axes.set_title(\"Most Popular Day for Airport Drop-offs (2020-2024)\", fontsize=14)\n",
    "    axes.set_xlabel(\"Day of the Week\", fontsize=12)\n",
    "    axes.set_ylabel(\"Number of Rides\", fontsize=12)\n",
    "    axes.legend(title=\"Airports\", fontsize=8)\n",
    " \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76b58a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# airport coordinates\n",
    "LGA_BOX_COORDS = ((40.763589, -73.891745), (40.778865, -73.854838))\n",
    "JFK_BOX_COORDS = ((40.639263, -73.795642), (40.651376, -73.766264))\n",
    "EWR_BOX_COORDS = ((40.686794, -74.194028), (40.699680, -74.165205))\n",
    "\n",
    "# Combine taxi and uber rides for drop off destination being LGA, JFK and EWR by day of the week\n",
    "\n",
    "def popular_day_to_airport_query():\n",
    "\n",
    "    QUERY_3 = \"\"\"\n",
    "    WITH Airports AS (\n",
    "        SELECT\n",
    "            *,\n",
    "            CASE\n",
    "                WHEN dropoff_latitude BETWEEN 40.763589 AND 40.778865\n",
    "                     AND dropoff_longitude BETWEEN -73.891745 AND -73.854838 THEN 'LGA'\n",
    "                WHEN dropoff_latitude BETWEEN 40.639263 AND 40.651376\n",
    "                     AND dropoff_longitude BETWEEN -73.795642 AND -73.766264 THEN 'JFK'\n",
    "                WHEN dropoff_latitude BETWEEN 40.686794 AND 40.699680\n",
    "                     AND dropoff_longitude BETWEEN -74.194028 AND -74.165205 THEN 'EWR'\n",
    "            END AS airport\n",
    "        FROM (\n",
    "            SELECT \n",
    "                dropoff_latitude,\n",
    "                dropoff_longitude,\n",
    "                strftime('%w', tpep_dropoff_datetime) AS day_of_week,\n",
    "                'Taxi' AS source\n",
    "            FROM taxi_trips\n",
    "            WHERE tpep_dropoff_datetime BETWEEN '2020-01-01 00:00:00' AND '2024-08-31 23:59:59'\n",
    "\n",
    "            UNION ALL\n",
    "\n",
    "            SELECT \n",
    "                dropoff_latitude,\n",
    "                dropoff_longitude,\n",
    "                strftime('%w', dropoff_datetime) AS day_of_week,\n",
    "                'Uber' AS source\n",
    "            FROM uber_trips\n",
    "            WHERE dropoff_datetime BETWEEN '2020-01-01 00:00:00' AND '2024-08-31 23:59:59'\n",
    "        )\n",
    "    )\n",
    "    SELECT \n",
    "        airport,\n",
    "        day_of_week,\n",
    "        COUNT(*) AS ride_count\n",
    "    FROM Airports\n",
    "    WHERE airport IS NOT NULL\n",
    "    GROUP BY airport, day_of_week\n",
    "    ORDER BY airport, day_of_week;\n",
    "    \"\"\"\n",
    "\n",
    "    with engine.connect() as con:\n",
    "        results = con.execute(db.text(QUERY_3)).fetchall()\n",
    "    \n",
    "    return pd.DataFrame(results, columns=[\"airport\", \"day_of_week\", \"ride_count\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17ea915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# execute query function and plot\n",
    "\n",
    "popular_day_to_airport = popular_day_to_airport_query()\n",
    "plot_popular_day_to_airport(popular_day_to_airport)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a862b1",
   "metadata": {},
   "source": [
    "### Visualization 4- How much do hired rides earn in total fares monthly?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95cc0dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show total fare breakdown by base, surcharges, taxes and tolls by month for uber and taxi rides respectively.\n",
    "\n",
    "def monthly_earning():\n",
    "\n",
    "    QUERY_4 = \"\"\"\n",
    "    WITH CombinedFares AS (\n",
    "        SELECT\n",
    "            strftime('%Y-%m', tpep_pickup_datetime) AS month,\n",
    "            SUM(fare_amount) AS base_fare,\n",
    "            SUM(extra + improvement_surcharge + congestion_surcharge + airport_fee) AS surcharges,\n",
    "            SUM(mta_tax) AS taxes,\n",
    "            SUM(tolls_amount) AS tolls,\n",
    "            'Taxi' AS source\n",
    "        FROM taxi_trips\n",
    "        WHERE tpep_pickup_datetime BETWEEN '2020-01-01 00:00:00' AND '2024-08-31 23:59:59'\n",
    "        GROUP BY month\n",
    "\n",
    "        UNION ALL\n",
    "\n",
    "        SELECT\n",
    "            strftime('%Y-%m', pickup_datetime) AS month,\n",
    "            SUM(base_passenger_fare) AS base_fare,\n",
    "            SUM(congestion_surcharge + bcf + airport_fee) AS surcharges,\n",
    "            SUM(sales_tax) AS taxes,\n",
    "            SUM(tolls) AS tolls,\n",
    "            'Uber' AS source\n",
    "        FROM uber_trips\n",
    "        WHERE pickup_datetime BETWEEN '2020-01-01 00:00:00' AND '2024-08-31 23:59:59'\n",
    "        GROUP BY month\n",
    "    )\n",
    "    SELECT\n",
    "        month,\n",
    "        source,\n",
    "        SUM(base_fare) AS base_fare,\n",
    "        SUM(surcharges) AS surcharges,\n",
    "        SUM(taxes) AS taxes,\n",
    "        SUM(tolls) AS tolls,\n",
    "        SUM(base_fare + surcharges + taxes + tolls) AS total_fare\n",
    "    FROM CombinedFares\n",
    "    GROUP BY month, source\n",
    "    ORDER BY month, source;\n",
    "    \"\"\"\n",
    "    \n",
    "    with engine.connect() as con:\n",
    "        results = con.execute(db.text(QUERY_4)).fetchall()\n",
    "    return pd.DataFrame(results, columns=[\"month\", \"source\", \"base_fare\", \"surcharges\", \"taxes\", \"tolls\", \"total_fare\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42d73ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a stack chart to show monthly earnings by base fare, surcharges and tolls \n",
    "# and by month for uber and taxi rides\n",
    "\n",
    "def plot_monthly_earning(dataframe):\n",
    "    \n",
    "    # Prepare data\n",
    "    dataframe['month'] = pd.to_datetime(dataframe['month'])\n",
    "    dataframe = dataframe.sort_values(by='month')\n",
    "    taxi_data = dataframe[dataframe['source'] == 'Taxi']\n",
    "    uber_data = dataframe[dataframe['source'] == 'Uber']\n",
    "\n",
    "    # Extract values for plotting\n",
    "    months = taxi_data['month'].dt.strftime('%Y-%m')\n",
    "    components = ['base_fare', 'surcharges', 'taxes', 'tolls']\n",
    "\n",
    "    # Create a grouped bar chart with stacked components\n",
    "    figure, axes = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "    bar_width = 0.4\n",
    "    index = range(len(months))\n",
    "\n",
    "    for i, component in enumerate(components):\n",
    "        \n",
    "        # Stack Taxi and Uber components\n",
    "        taxi_bottom = taxi_data[components[:i]].sum(axis=1) if i > 0 else 0\n",
    "        uber_bottom = uber_data[components[:i]].sum(axis=1) if i > 0 else 0\n",
    "\n",
    "        # Taxi bars\n",
    "        axes.bar(\n",
    "            [x - bar_width / 2 for x in index],\n",
    "            taxi_data[component],\n",
    "            width=bar_width,\n",
    "            bottom=taxi_bottom,\n",
    "            label=f\"Taxi - {component.capitalize()}\",\n",
    "            alpha=0.7\n",
    "        )\n",
    "\n",
    "        # Uber bars\n",
    "        axes.bar(\n",
    "            [x + bar_width / 2 for x in index],\n",
    "            uber_data[component],\n",
    "            width=bar_width,\n",
    "            bottom=uber_bottom,\n",
    "            label=f\"Uber - {component.capitalize()}\",\n",
    "            alpha=0.7\n",
    "        )\n",
    "\n",
    "    # Adding labels, title, and styling\n",
    "    axes.set_title(\"Monthly Earnings for Hired Rides (2020-2024)\", fontsize=16)\n",
    "    axes.set_xlabel(\"Month\", fontsize=14)\n",
    "    axes.set_ylabel(\"Earnings (USD)\", fontsize=14)\n",
    "    axes.set_xticks(index)\n",
    "    axes.set_xticklabels(months, fontsize=10, rotation=45)\n",
    "    axes.legend(title=\"Earning Components\", fontsize=10)\n",
    "\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b93352",
   "metadata": {},
   "outputs": [],
   "source": [
    "# execute query function and plot\n",
    "\n",
    "monthly_earning = monthly_earning()\n",
    "plot_monthly_earning(monthly_earning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7fc30b",
   "metadata": {},
   "source": [
    "### Visualization 5 - Does precipitation or distance traveled affect the amount of tip?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c292dbd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create query to:\n",
    "# 1) compare uber tip amount with distance \n",
    "# 2) compare taxi tip amount with distance  \n",
    "# 3) compare uber tip with precipatation by joining weather and uber tables\n",
    "# 4) compare taxi tip with precipatation by joining weather and taxi tables\n",
    "\n",
    "def precipitation_distance_vs_tip():\n",
    "    QUERY_5 = \"\"\"\n",
    "    WITH TaxiFiltered AS (\n",
    "        SELECT\n",
    "            trip_distance AS distance,\n",
    "            tip_amount AS tip,\n",
    "            d.DailyPrecipitation AS precipitation\n",
    "        FROM taxi_trips t\n",
    "        LEFT JOIN daily_weather d\n",
    "            ON DATE(t.tpep_pickup_datetime) = DATE(d.DATE)\n",
    "        WHERE t.tpep_pickup_datetime BETWEEN '2022-01-01' AND '2023-12-31'\n",
    "          AND trip_distance > 0\n",
    "          AND tip_amount >= 0\n",
    "    ),\n",
    "    UberFiltered AS (\n",
    "        SELECT\n",
    "            trip_miles AS distance,\n",
    "            tips AS tip,\n",
    "            d.DailyPrecipitation AS precipitation\n",
    "        FROM uber_trips u\n",
    "        LEFT JOIN daily_weather d\n",
    "            ON DATE(u.pickup_datetime) = DATE(d.DATE)\n",
    "        WHERE u.pickup_datetime BETWEEN '2022-01-01' AND '2023-12-31'\n",
    "          AND trip_miles > 0\n",
    "          AND tips >= 0\n",
    "    )\n",
    "    SELECT 'Taxi' AS source, distance, tip, precipitation\n",
    "    FROM TaxiFiltered\n",
    "    UNION ALL\n",
    "    SELECT 'Uber' AS source, distance, tip, precipitation\n",
    "    FROM UberFiltered;\n",
    "    \"\"\"\n",
    "    # Fetch the data from the database\n",
    "    with engine.connect() as con:\n",
    "        results = con.execute(db.text(QUERY_5)).fetchall()\n",
    "    return pd.DataFrame(results, columns=[\"source\", \"distance\", \"tip\", \"precipitation\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcde60ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_precipitation_distance_vs_tip(dataframe):\n",
    "    \n",
    "    # Separate the data by source\n",
    "    taxi_data = dataframe[dataframe[\"source\"] == \"Taxi\"]\n",
    "    uber_data = dataframe[dataframe[\"source\"] == \"Uber\"]\n",
    "\n",
    "    # Remove outliers to limit to 95th percentile of distance and tip\n",
    "    for data in [taxi_data, uber_data]:\n",
    "        data = data[\n",
    "            (data[\"distance\"] <= data[\"distance\"].quantile(0.95)) &\n",
    "            (data[\"tip\"] <= data[\"tip\"].quantile(0.95))\n",
    "        ]\n",
    "\n",
    "    # Create the 2x2 subplot figure\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    fig.suptitle(\"Effect of Precipitation and Distance on Tip Amount (2022-2023)\", fontsize=16)\n",
    "\n",
    "    # Plot 1: Taxi - Tip vs Distance\n",
    "    axes[0, 0].scatter(taxi_data[\"distance\"], taxi_data[\"tip\"])\n",
    "    axes[0, 0].set_title(\"Taxi: Tip vs Distance\")\n",
    "    axes[0, 0].set_xlabel(\"Distance (miles)\")\n",
    "    axes[0, 0].set_ylabel(\"Tip Amount)\")\n",
    "\n",
    "    # Plot 2: Uber - Tip vs Distance\n",
    "    axes[0, 1].scatter(uber_data[\"distance\"], uber_data[\"tip\"])\n",
    "    axes[0, 1].set_title(\"Uber: Tip vs Distance\")\n",
    "    axes[0, 1].set_xlabel(\"Distance (miles)\")\n",
    "    axes[0, 1].set_ylabel(\"Tip Amount\")\n",
    "\n",
    "    # Plot 3: Taxi - Tip vs Precipitation\n",
    "    axes[1, 0].scatter(taxi_data[\"precipitation\"], taxi_data[\"tip\"])\n",
    "    axes[1, 0].set_title(\"Taxi: Tip vs Precipitation\")\n",
    "    axes[1, 0].set_xlabel(\"Precipitation (inches)\")\n",
    "    axes[1, 0].set_ylabel(\"Tip Amount\")\n",
    "\n",
    "    # Plot 4: Uber - Tip vs Precipitation\n",
    "    axes[1, 1].scatter(uber_data[\"precipitation\"], uber_data[\"tip\"])\n",
    "    axes[1, 1].set_title(\"Uber: Tip vs Precipitation\")\n",
    "    axes[1, 1].set_xlabel(\"Precipitation (inches)\")\n",
    "    axes[1, 1].set_ylabel(\"Tip Amount\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c54773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# execute query function and plot\n",
    "precipitation_distance_vs_tip = precipitation_distance_vs_tip()\n",
    "plot_precipitation_distance_vs_tip(precipitation_distance_vs_tip)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39e4a81",
   "metadata": {},
   "source": [
    "### Visualization 6 - Create a heatmap of all hired trips in 2020 over a map of the area. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e6fbf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot to create the heatmap for taxi and uber trips in 2020 in New York City\n",
    "\n",
    "def plot_heatmap_all_trips_2020(dataframe, map_center, output_html):\n",
    "    \n",
    "    # Combine pickup and dropoff points into one DataFrame\n",
    "    combined_data = pd.concat([\n",
    "        dataframe[['pickup_latitude', 'pickup_longitude']].rename(\n",
    "            columns={'pickup_latitude': 'latitude', 'pickup_longitude': 'longitude'}),\n",
    "        dataframe[['dropoff_latitude', 'dropoff_longitude']].rename(\n",
    "            columns={'dropoff_latitude': 'latitude', 'dropoff_longitude': 'longitude'})\n",
    "    ], ignore_index=True)\n",
    "    \n",
    "    # Drop missing values\n",
    "    combined_data = combined_data.dropna(subset=['latitude', 'longitude'])\n",
    "\n",
    "    # Filter data to focus on NYC area\n",
    "    nyc_bbox = {\n",
    "        \"min_lat\": 40.560445,\n",
    "        \"max_lat\": 40.908524,\n",
    "        \"min_lon\": -74.242330,\n",
    "        \"max_lon\": -73.717047,\n",
    "    }\n",
    "    filtered_data = combined_data[\n",
    "        (combined_data['latitude'].between(nyc_bbox['min_lat'], nyc_bbox['max_lat'])) &\n",
    "        (combined_data['longitude'].between(nyc_bbox['min_lon'], nyc_bbox['max_lon']))\n",
    "    ]\n",
    "\n",
    "    # Prepare data for the heatmap\n",
    "    heat_data = filtered_data[['latitude', 'longitude']].values.tolist()\n",
    "\n",
    "    # Create a Folium map\n",
    "    m = folium.Map(location=map_center, zoom_start=12)\n",
    "\n",
    "    # Add a heatmap layer\n",
    "    HeatMap(heat_data, radius=8, blur=10, max_zoom=12).add_to(m)\n",
    "\n",
    "    # Save map to HTML file\n",
    "    m.save(output_html)\n",
    "    print(f\"Heatmap saved to {output_html}\")\n",
    "\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0e9b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine uber and taxi trips in 2020\n",
    "\n",
    "def heatmap_all_trips_2020():\n",
    "    QUERY_6 = \"\"\"\n",
    "    SELECT \n",
    "        pickup_latitude, \n",
    "        pickup_longitude, \n",
    "        dropoff_latitude, \n",
    "        dropoff_longitude \n",
    "    FROM taxi_trips\n",
    "    WHERE tpep_pickup_datetime BETWEEN '2020-01-01 00:00:00' AND '2020-12-31 23:59:59'\n",
    "    UNION ALL\n",
    "    SELECT \n",
    "        pickup_latitude, \n",
    "        pickup_longitude, \n",
    "        dropoff_latitude, \n",
    "        dropoff_longitude \n",
    "    FROM uber_trips\n",
    "    WHERE pickup_datetime BETWEEN '2020-01-01 00:00:00' AND '2020-12-31 23:59:59'\n",
    "    \"\"\"\n",
    "    \n",
    "    # Connect to database and fetch data\n",
    "    with engine.connect() as con:\n",
    "        results = con.execute(db.text(QUERY_6)).fetchall()\n",
    "    trip_data = pd.DataFrame(results, columns=[\"pickup_latitude\", \"pickup_longitude\", \"dropoff_latitude\", \"dropoff_longitude\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b14cf70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the heatmap\n",
    "\n",
    "nyc_center = (40.7128, -74.0060)  # Approximate center of NYC\n",
    "output_file = \"nyc_heatmap_2020.html\"\n",
    "plot_heatmap_all_trips_2020(trip_data, nyc_center, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90615be1",
   "metadata": {},
   "source": [
    "### Extra credit: Do majority of the uber rides happen before or after sunset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2511cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a daily sun times table with date, sunset and sunrise columns from daily_weather_data\n",
    "\n",
    "def create_daily_sun_times_table(engine, daily_weather_data):\n",
    "    \"\"\"\n",
    "    Create a SQL table for daily sunrise and sunset times from weather data.\n",
    "    \"\"\"\n",
    "    daily_weather_data[['DATE', 'Sunrise', 'Sunset']].to_sql(\n",
    "        'daily_sun_times',\n",
    "        con=engine,\n",
    "        if_exists='replace',\n",
    "        index=False,\n",
    "        dtype={\n",
    "            'DATE': db.types.Date,\n",
    "            'Sunrise': db.types.Time,\n",
    "            'Sunset': db.types.Time,\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294856fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL Query for Uber rides categorized by time of day\n",
    "QUERY_UBER_BEFORE_AFTER_SUNSET = \"\"\"\n",
    "WITH UberWithSunTimes AS (\n",
    "    SELECT \n",
    "        u.pickup_datetime,\n",
    "        DATE(u.pickup_datetime) AS ride_date,\n",
    "        CASE \n",
    "            WHEN TIME(u.pickup_datetime) < TIME(w.Sunrise) THEN 'Before Sunrise'\n",
    "            WHEN TIME(u.pickup_datetime) > TIME(w.Sunset) THEN 'After Sunset'\n",
    "            ELSE 'Daytime'\n",
    "        END AS time_period\n",
    "    FROM uber_trips u\n",
    "    JOIN daily_sun_times w\n",
    "    ON DATE(u.pickup_datetime) = w.DATE\n",
    ")\n",
    "SELECT \n",
    "    time_period,\n",
    "    COUNT(*) AS ride_count\n",
    "FROM UberWithSunTimes\n",
    "GROUP BY time_period\n",
    "ORDER BY ride_count DESC;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb106616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to get ride counts by time period\n",
    "def get_uber_time_period_data(engine):\n",
    "\n",
    "    with engine.connect() as con:\n",
    "        results = con.execute(db.text(QUERY_UBER_BEFORE_AFTER_SUNSET)).fetchall()\n",
    "    return pd.DataFrame(results, columns=[\"time_period\", \"ride_count\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a8322d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the Uber ride counts categorized by time periods using bar chart.\n",
    "   \n",
    "def plot_uber_time_period(dataframe):\n",
    "    \"\"\"\n",
    "    Visualize the Uber ride counts categorized by time periods.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    ax.bar(dataframe['time_period'], dataframe['ride_count'], color=['#4CAF50', '#FF5733', '#1E90FF'])\n",
    "\n",
    "    ax.set_title(\"Uber Rides Before Sunrise, After Sunset, and During Daytime\", fontsize=16)\n",
    "    ax.set_xlabel(\"Time Period\", fontsize=14)\n",
    "    ax.set_ylabel(\"Number of Rides\", fontsize=14)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22075da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the daily_sun_times table\n",
    "create_daily_sun_times_table(engine, daily_weather_data)\n",
    "\n",
    "# Step 2: Execute the query to get ride counts\n",
    "uber_time_period_df = get_uber_time_period_data(engine)\n",
    "\n",
    "#  Plot the results\n",
    "plot_uber_time_period(uber_time_period_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
